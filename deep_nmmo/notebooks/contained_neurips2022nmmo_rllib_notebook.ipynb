{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628fa1e6-327f-4ecd-8534-88da25f978bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 20:42:57,513\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8269 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import deep_nmmo\n",
    "from deep_nmmo.envs.team_based_env.teams.custom_team import CustomTeam\n",
    "from deep_nmmo.utils import get_class_from_path, get_module_from_path\n",
    "from deep_nmmo.envs.team_based_env.env_configs.custom_competition_config import CustomCompetitionConfig\n",
    "from deep_nmmo.envs.team_based_env.loops.utils import init_env_params, reset_teams, reset_env\n",
    "\n",
    "import nmmo\n",
    "from nmmo import config\n",
    "from nmmo.io import action\n",
    "from nmmo import scripting, material, Serialized\n",
    "from nmmo.systems import skill, item\n",
    "from nmmo.lib import colors\n",
    "from nmmo import action as Action\n",
    "\n",
    "import neurips2022nmmo\n",
    "from neurips2022nmmo.scripted import baselines\n",
    "from neurips2022nmmo import Team\n",
    "from neurips2022nmmo import CompetitionConfig, scripted, RollOut, TeamBasedEnv\n",
    "from neurips2022nmmo.scripted import attack, move\n",
    "\n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "# from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from ray.rllib.models.modelv2 import restore_original_dimensions\n",
    "from collections.abc import Mapping\n",
    "\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.typing import (\n",
    "    # AgentID,\n",
    "    # EnvCreator,\n",
    "    # EnvID,\n",
    "    # EnvType,\n",
    "    MultiAgentDict,\n",
    "    # MultiEnvDict,\n",
    ")\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from typing import Dict, Any, Type, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "028da21d-1fa2-46f4-8817-e7173dd3ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "from deep_nmmo.envs.rllib_multi_agent_team_based_env.env import RLlibMultiAgentTeamBasedEnv\n",
    "from deep_nmmo.envs.rllib_multi_agent_team_based_env.agents.rllib_random_policy import RLlibRandomPolicy\n",
    "\n",
    "\n",
    "path_to_rllib_trainer_cls = 'ray.rllib.agents.ppo.PPOTrainer'\n",
    "\n",
    "path_to_env_config_cls = 'deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig'\n",
    "\n",
    "num_players_per_team = 8\n",
    "# will overwrite agents kwargs with observation and action space and player index when initialising teams in RLlib env\n",
    "custom_team_kwargs = {'agents_cls': [RLlibRandomPolicy for _ in range(num_players_per_team)], \n",
    "                      'agents_kwargs': [None for _ in range(num_players_per_team)],\n",
    "                     }\n",
    "\n",
    "teams_config = {\n",
    "    # 'RLlib': {'cls': RLlibScriptedHybridAgentTeam, 'kwargs': {'paths_to_scripted_agents_cls': rllib_paths_to_scripted_agents_cls}},\n",
    "    \n",
    "    # 'Combat-1': {'cls': neurips2022nmmo.scripted.CombatTeam, 'kwargs': {'team_id': 1}},\n",
    "    # 'Combat-2': {'cls': neurips2022nmmo.scripted.CombatTeam, 'kwargs': {'team_id': 2}},\n",
    "    # 'Mixture-1': {'cls': neurips2022nmmo.scripted.MixtureTeam, 'kwargs': {'team_id': 3}},\n",
    "    \n",
    "    'R-1': {'cls': deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam, 'kwargs': custom_team_kwargs},\n",
    "    'R-2': {'cls': deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam, 'kwargs': custom_team_kwargs},\n",
    "    'R-3': {'cls': deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam, 'kwargs': custom_team_kwargs},\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "env_config = get_class_from_path(path_to_env_config_cls)()\n",
    "\n",
    "policies = {}\n",
    "policy_id = 1\n",
    "num_teams, num_players_per_team = 3, 8\n",
    "for _ in range(num_teams):\n",
    "    player_idx = 0\n",
    "    for _ in range(num_players_per_team):\n",
    "        policies[str(policy_id)] = PolicySpec(\n",
    "                                    policy_class=RLlibRandomPolicy, # infer from env\n",
    "                                    # observation_space=dummy_env.observation_space(player_id),\n",
    "                                    # action_space=dummy_env.action_space(player_id),\n",
    "                                    config={'env_config': env_config, 'idx': player_idx},\n",
    "                                )\n",
    "        policy_id += 1\n",
    "        player_idx += 1\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    '''Maps agent ID (int) to corresponding policy ID (str) of policy which agent should use.'''\n",
    "    print(f'>>> Mapping agent_id {agent_id} to {str(agent_id)}')\n",
    "    return str(agent_id)\n",
    "\n",
    "multiagent_config = {\n",
    "    'policies': policies,\n",
    "    'policy_mapping_fn': policy_mapping_fn,\n",
    "    'policies_to_train': [],\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    # 'path_to_env_cls': path_to_env_cls,\n",
    "    # 'path_to_env_config_cls': path_to_env_config_cls,\n",
    "    'env_config': env_config,\n",
    "    'teams_config': teams_config,\n",
    "}\n",
    "\n",
    "\n",
    "rllib_config = {\n",
    "    'env': 'ma_env',\n",
    "    \n",
    "    'env_config': env_config,\n",
    "    \n",
    "    'disable_env_checking': True,\n",
    "    # 'disable_env_checking': False,\n",
    "    \n",
    "    'framework': 'torch',\n",
    "    \n",
    "    'multiagent': multiagent_config,\n",
    "    \n",
    "    'num_workers': 0,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605ef967-9fb2-4018-b242-0a5664474ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# register env with ray\n",
    "register_env('ma_env', lambda env_config: RLlibMultiAgentTeamBasedEnv(**env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6c3e4d-fa0a-4ae3-9c57-daa12f4b58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 20:58:59,004\tWARNING multi_agent.py:121 -- `config.multiagent.policies_to_train` is empty! Make sure - if you would like to learn at least one policy - to add its ID to that list.\n",
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 13.45it/s]\n",
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 13.93it/s]\n",
      "2022-09-29 20:59:07,151\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised trainer\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# merge rllibg trainer's default config with specified config\n",
    "path_to_agent = '.'.join(path_to_rllib_trainer_cls.split('.')[:-1])\n",
    "_rllib_config = get_module_from_path(path_to_agent).DEFAULT_CONFIG.copy()\n",
    "_rllib_config.update(rllib_config)\n",
    "# print(_rllib_config)\n",
    "\n",
    "# init rllib trainer\n",
    "trainer = get_class_from_path(path_to_rllib_trainer_cls)(config=_rllib_config)\n",
    "print(f'Initialised trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9657f7-3733-4f15-b702-9268fb943d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~ RESET ~~~\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 24 dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])\n",
      ">>> Mapping agent_id 1 to 1\n",
      ">>> Mapping agent_id 2 to 2\n",
      ">>> Mapping agent_id 3 to 3\n",
      ">>> Mapping agent_id 4 to 4\n",
      ">>> Mapping agent_id 5 to 5\n",
      ">>> Mapping agent_id 6 to 6\n",
      ">>> Mapping agent_id 7 to 7\n",
      ">>> Mapping agent_id 8 to 8\n",
      ">>> Mapping agent_id 9 to 9\n",
      ">>> Mapping agent_id 10 to 10\n",
      ">>> Mapping agent_id 11 to 11\n",
      ">>> Mapping agent_id 12 to 12\n",
      ">>> Mapping agent_id 13 to 13\n",
      ">>> Mapping agent_id 14 to 14\n",
      ">>> Mapping agent_id 15 to 15\n",
      ">>> Mapping agent_id 16 to 16\n",
      ">>> Mapping agent_id 17 to 17\n",
      ">>> Mapping agent_id 18 to 18\n",
      ">>> Mapping agent_id 19 to 19\n",
      ">>> Mapping agent_id 20 to 20\n",
      ">>> Mapping agent_id 21 to 21\n",
      ">>> Mapping agent_id 22 to 22\n",
      ">>> Mapping agent_id 23 to 23\n",
      ">>> Mapping agent_id 24 to 24\n",
      "\n",
      "~~~ Step 1 ~~~\n",
      "action_dict keys: 24 dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 24 dict_keys([1, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 2, 4, 5, 22])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 24 {1: 0, 3: 0, 6: 0, 7: 0, 8: 0, 2: -1, 4: -1, 5: -1, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 23: 0, 24: 0, 22: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 25 {1: False, 3: False, 6: False, 7: False, 8: False, 2: True, 4: True, 5: True, 9: False, 10: False, 11: False, 12: False, 13: False, 14: False, 15: False, 16: False, 17: False, 18: False, 19: False, 20: False, 21: False, 23: False, 24: False, 22: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 24 dict_keys([1, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 2, 4, 5, 22])\n",
      "\n",
      "~~~ Step 2 ~~~\n",
      "action_dict keys: 20 dict_keys([1, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 20 dict_keys([1, 3, 7, 10, 11, 12, 14, 17, 19, 20, 24, 6, 8, 9, 13, 15, 16, 18, 21, 23])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 20 {1: 0, 3: 0, 7: 0, 6: -1, 8: -1, 10: 0, 11: 0, 12: 0, 14: 0, 9: -1, 13: -1, 15: -1, 16: -1, 17: 0, 19: 0, 20: 0, 24: 0, 18: -1, 21: -1, 23: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 21 {1: False, 3: False, 7: False, 6: True, 8: True, 10: False, 11: False, 12: False, 14: False, 9: True, 13: True, 15: True, 16: True, 17: False, 19: False, 20: False, 24: False, 18: True, 21: True, 23: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 20 dict_keys([1, 3, 7, 10, 11, 12, 14, 17, 19, 20, 24, 6, 8, 9, 13, 15, 16, 18, 21, 23])\n",
      "\n",
      "~~~ Step 3 ~~~\n",
      "action_dict keys: 11 dict_keys([1, 3, 7, 10, 11, 12, 14, 17, 19, 20, 24])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 11 dict_keys([1, 3, 7, 12, 14, 17, 19, 24, 10, 11, 20])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 11 {1: 0, 3: 0, 7: 0, 12: 0, 14: 0, 10: -1, 11: -1, 17: 0, 19: 0, 24: 0, 20: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 12 {1: False, 3: False, 7: False, 12: False, 14: False, 10: True, 11: True, 17: False, 19: False, 24: False, 20: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 11 dict_keys([1, 3, 7, 12, 14, 17, 19, 24, 10, 11, 20])\n",
      "\n",
      "~~~ Step 4 ~~~\n",
      "action_dict keys: 8 dict_keys([1, 3, 7, 12, 14, 17, 19, 24])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 8 dict_keys([3, 7, 12, 14, 17, 19, 24, 1])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 8 {3: 0, 7: 0, 1: -1, 12: 0, 14: 0, 17: 0, 19: 0, 24: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 9 {3: False, 7: False, 1: True, 12: False, 14: False, 17: False, 19: False, 24: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 8 dict_keys([3, 7, 12, 14, 17, 19, 24, 1])\n",
      "\n",
      "~~~ Step 5 ~~~\n",
      "action_dict keys: 7 dict_keys([3, 7, 12, 14, 17, 19, 24])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 7 dict_keys([3, 7, 12, 14, 17, 19, 24])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 7 {3: 0, 7: 0, 12: 0, 14: 0, 17: 0, 19: 0, 24: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 8 {3: False, 7: False, 12: False, 14: False, 17: False, 19: False, 24: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 7 dict_keys([3, 7, 12, 14, 17, 19, 24])\n",
      "\n",
      "~~~ Step 6 ~~~\n",
      "action_dict keys: 7 dict_keys([3, 7, 12, 14, 17, 19, 24])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 7 dict_keys([3, 7, 12, 14, 17, 19, 24])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 7 {3: 0, 7: 0, 12: 0, 14: 0, 17: 0, 19: 0, 24: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 8 {3: False, 7: False, 12: False, 14: False, 17: False, 19: False, 24: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 7 dict_keys([3, 7, 12, 14, 17, 19, 24])\n",
      "\n",
      "~~~ Step 7 ~~~\n",
      "action_dict keys: 6 dict_keys([3, 7, 12, 14, 17, 19])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 6 dict_keys([3, 7, 12, 17, 19, 14])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 6 {3: 0, 7: 0, 12: 0, 14: -1, 17: 0, 19: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 7 {3: False, 7: False, 12: False, 14: True, 17: False, 19: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 6 dict_keys([3, 7, 12, 17, 19, 14])\n",
      "\n",
      "~~~ Step 8 ~~~\n",
      "action_dict keys: 5 dict_keys([3, 7, 12, 17, 19])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 5 dict_keys([3, 7, 12, 19, 17])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 5 {3: 0, 7: 0, 12: 0, 19: 0, 17: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 6 {3: False, 7: False, 12: False, 19: False, 17: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 5 dict_keys([3, 7, 12, 19, 17])\n",
      "\n",
      "~~~ Step 9 ~~~\n",
      "action_dict keys: 4 dict_keys([3, 7, 12, 19])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 4 dict_keys([3, 7, 12, 19])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 4 {3: 0, 7: 0, 12: 0, 19: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 5 {3: False, 7: False, 12: False, 19: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 4 dict_keys([3, 7, 12, 19])\n",
      "\n",
      "~~~ Step 10 ~~~\n",
      "action_dict keys: 4 dict_keys([3, 7, 12, 19])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 4 dict_keys([3, 7, 19, 12])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 4 {3: 0, 7: 0, 12: -1, 19: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 5 {3: False, 7: False, 12: True, 19: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 4 dict_keys([3, 7, 19, 12])\n",
      "\n",
      "~~~ Step 11 ~~~\n",
      "action_dict keys: 3 dict_keys([3, 7, 19])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 3 dict_keys([3, 7, 19])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 3 {3: 0, 7: 0, 19: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 4 {3: False, 7: False, 19: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 3 dict_keys([3, 7, 19])\n",
      "\n",
      "~~~ Step 12 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 13 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 14 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 15 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 16 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 17 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 18 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 19 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 20 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 21 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 22 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 23 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 24 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 25 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 26 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 27 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 28 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 29 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 30 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 31 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 32 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 33 ~~~\n",
      "action_dict keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 2 dict_keys([3, 7])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 2 {3: 0, 7: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 3 {3: False, 7: True, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 2 dict_keys([3, 7])\n",
      "\n",
      "~~~ Step 34 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 35 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 36 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 37 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 38 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 39 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 40 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 41 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 42 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 43 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: 0}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: False, '__all__': False}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n",
      "\n",
      "~~~ Step 44 ~~~\n",
      "action_dict keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv obs keys: 1 dict_keys([3])\n",
      "RLlibMultiAgentTeamBasedEnv rew: 1 {3: -1}\n",
      "RLlibMultiAgentTeamBasedEnv done: 2 {3: True, '__all__': True}\n",
      "RLlibMultiAgentTeamBasedEnv info keys: 1 dict_keys([3])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# perform one training epoch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleted RLlib epoch!!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m     (\n\u001b[1;32m    654\u001b[0m         results,\n\u001b[1;32m    655\u001b[0m         train_iter_ctx,\n\u001b[1;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2378\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[38;5;66;03m# In case of any failures, try to ignore/recover the failed workers.\u001b[39;00m\n\u001b[1;32m   2377\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2378\u001b[0m             num_recreated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_recover_from_step_attempt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m                \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m                \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m                \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_worker_failures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrecreate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecreate_failed_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2384\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_recreated_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_recreated\n\u001b[1;32m   2386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2190\u001b[0m, in \u001b[0;36mAlgorithm.try_recover_from_step_attempt\u001b[0;34m(self, error, worker_set, ignore, recreate)\u001b[0m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;66;03m# Any other exception.\u001b[39;00m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m-> 2190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   2192\u001b[0m removed_workers, new_workers \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;66;03m# Search for failed workers and try to recover (restart) them.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    404\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py:97\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m     max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m<\u001b[39m max_agent_or_env_steps\n\u001b[1;32m     93\u001b[0m ):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# samples.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mremote_workers():\n\u001b[0;32m---> 97\u001b[0m         sample_batches \u001b[38;5;241m=\u001b[39m [\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         sample_batches \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    101\u001b[0m             [worker\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39mremote() \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mremote_workers()]\n\u001b[1;32m    102\u001b[0m         )\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:806\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_start\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    800\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample batch of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_fragment_length\n\u001b[1;32m    803\u001b[0m         )\n\u001b[1;32m    804\u001b[0m     )\n\u001b[0;32m--> 806\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    807\u001b[0m steps_so_far \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    808\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magent_steps()\n\u001b[1;32m    811\u001b[0m )\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:92\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@override\u001b[39m(InputReader)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[0;32m---> 92\u001b[0m     batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     93\u001b[0m     batches\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extra_batches())\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:282\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@override\u001b[39m(SamplerInput)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_runner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_queue\u001b[38;5;241m.\u001b[39mput(item)\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:684\u001b[0m, in \u001b[0;36m_env_runner\u001b[0;34m(worker, base_env, extra_batch_callback, horizon, normalize_actions, clip_actions, multiple_episodes_in_batch, callbacks, perf_stats, soft_horizon, no_done_at_end, observation_fn, sample_collector, render)\u001b[0m\n\u001b[1;32m    681\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    682\u001b[0m \u001b[38;5;66;03m# types: Set[EnvID], Dict[PolicyID, List[_PolicyEvalData]],\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m#       List[Union[RolloutMetrics, SampleBatchType]]\u001b[39;00m\n\u001b[0;32m--> 684\u001b[0m active_envs, to_eval, outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_process_observations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactive_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43munfiltered_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munfiltered_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiple_episodes_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiple_episodes_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoft_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoft_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_done_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_done_at_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m perf_stats\u001b[38;5;241m.\u001b[39mincr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_obs_processing_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:1041\u001b[0m, in \u001b[0;36m_process_observations\u001b[0;34m(worker, base_env, active_episodes, unfiltered_obs, rewards, dones, infos, horizon, multiple_episodes_in_batch, callbacks, soft_horizon, no_done_at_end, observation_fn, sample_collector)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# If, we are not allowed to pack the next episode into the same\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# SampleBatch (batch_mode=complete_episodes) -> Build the\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# MultiAgentBatch from a single episode and add it to \"outputs\".\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m# (to e.g. properly flush and clean up the SampleCollector's buffers),\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# but then discard the entire batch and don't return it.\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mis_faulty \u001b[38;5;129;01mor\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1041\u001b[0m     ma_sample_batch \u001b[38;5;241m=\u001b[39m \u001b[43msample_collector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhit_horizon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msoft_horizon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_dones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_dones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_faulty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmultiple_episodes_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mis_faulty:\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ma_sample_batch:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/collectors/simple_list_collector.py:435\u001b[0m, in \u001b[0;36mSimpleListCollector.postprocess_episode\u001b[0;34m(self, episode, is_done, check_dones, build)\u001b[0m\n\u001b[1;32m    433\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_key_to_policy_id[(eps_id, agent_id)]\n\u001b[1;32m    434\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_map[pid]\n\u001b[0;32m--> 435\u001b[0m     pre_batch \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview_requirements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     pre_batches[agent_id] \u001b[38;5;241m=\u001b[39m (policy, pre_batch)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# Apply reward clipping before calling postprocessing functions.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/collectors/agent_collector.py:395\u001b[0m, in \u001b[0;36mAgentCollector.build_for_training\u001b[0;34m(self, view_requirements)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# in some multi-agent cases shifted_data may be an empty list.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# In this case we should just create an empty array and return it.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shifted_data:\n\u001b[0;32m--> 395\u001b[0m     shifted_data_np \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshifted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     shifted_data_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(shifted_data)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/numpy/core/shape_base.py:426\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall input arrays must have the same shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    428\u001b[0m result_ndim \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    429\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# perform one training epoch\n",
    "results = trainer.train()\n",
    "print(f'Completed RLlib epoch!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91854a8-d93b-471f-8c68-dc6ca786dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c44f2d-de31-4e0d-91d7-4262446fc52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmmo",
   "language": "python",
   "name": "nmmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
