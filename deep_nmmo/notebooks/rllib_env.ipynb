{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "80d450f6-d874-4883-ad86-df7f87dc0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 20:01:03,181\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from deep_nmmo.envs.team_based_env.teams.custom_team import CustomTeam\n",
    "from deep_nmmo.utils import get_class_from_path, get_module_from_path\n",
    "from deep_nmmo.envs.team_based_env.env_configs.custom_competition_config import CustomCompetitionConfig\n",
    "from deep_nmmo.envs.team_based_env.loops.utils import init_env_params, reset_teams, reset_env\n",
    "\n",
    "import nmmo\n",
    "from nmmo import config\n",
    "from nmmo.io import action\n",
    "from nmmo import scripting, material, Serialized\n",
    "from nmmo.systems import skill, item\n",
    "from nmmo.lib import colors\n",
    "from nmmo import action as Action\n",
    "\n",
    "\n",
    "import neurips2022nmmo\n",
    "from neurips2022nmmo.scripted import baselines\n",
    "from neurips2022nmmo import Team\n",
    "from neurips2022nmmo import CompetitionConfig, scripted, RollOut, TeamBasedEnv\n",
    "from neurips2022nmmo.scripted import attack, move\n",
    "\n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "# from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.typing import (\n",
    "    # AgentID,\n",
    "    # EnvCreator,\n",
    "    # EnvID,\n",
    "    # EnvType,\n",
    "    MultiAgentDict,\n",
    "    # MultiEnvDict,\n",
    ")\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from typing import Dict, Any, Type, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b06c5-7760-4a2d-9ac6-8c6e1b09e7c6",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Create RLLibScriptedBaseline class to wrap around neurips2022nmmo.scripted.baselines agents so that agents are compatible with RLlib environments. Should inherit from RLlib Policy base class\n",
    "     - May need to have wrapper class for Team as well. Need to ensure all this still works when making submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d0239057-a14d-43f5-a8e6-0963e0614278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 14.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 15.51it/s]\n",
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 15.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space: Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32)))\n",
      "action_space: Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))\n",
      "Teams: [<deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7e965ceaea90>, <deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7e965ceaeac0>, <deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7e9656d41a30>]\n",
      "env_config.PLAYERS[0]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "env_config.PLAYERS[1]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "env_config.PLAYERS[2]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "env_config: <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7e965db10280>\n",
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_env: <neurips2022nmmo.env.team_based_env.TeamBasedEnv object at 0x7e965c2ba5e0>\n",
      "players: {1: <nmmo.entity.player.Player object at 0x7e96549861f0>, 2: <nmmo.entity.player.Player object at 0x7e9654d1d130>, 3: <nmmo.entity.player.Player object at 0x7e9654ebd070>, 4: <nmmo.entity.player.Player object at 0x7e9654ebdf70>, 5: <nmmo.entity.player.Player object at 0x7e9654ea5eb0>, 6: <nmmo.entity.player.Player object at 0x7e965517fdf0>, 7: <nmmo.entity.player.Player object at 0x7e965516cd30>, 8: <nmmo.entity.player.Player object at 0x7e96554f4c70>, 9: <nmmo.entity.player.Player object at 0x7e96554c2bb0>, 10: <nmmo.entity.player.Player object at 0x7e9655df6af0>, 11: <nmmo.entity.player.Player object at 0x7e9655fb9a30>, 12: <nmmo.entity.player.Player object at 0x7e9655fa2970>, 13: <nmmo.entity.player.Player object at 0x7e96561d48b0>, 14: <nmmo.entity.player.Player object at 0x7e96563f77f0>, 15: <nmmo.entity.player.Player object at 0x7e96563e8730>, 16: <nmmo.entity.player.Player object at 0x7e965667f670>, 17: <nmmo.entity.player.Player object at 0x7e96566455b0>, 18: <nmmo.entity.player.Player object at 0x7e96569684f0>, 19: <nmmo.entity.player.Player object at 0x7e9656949430>, 20: <nmmo.entity.player.Player object at 0x7e96545e4370>, 21: <nmmo.entity.player.Player object at 0x7e96546fe2b0>, 22: <nmmo.entity.player.Player object at 0x7e96546f01f0>, 23: <nmmo.entity.player.Player object at 0x7e96546ca130>, 24: <nmmo.entity.player.Player object at 0x7e965496d070>}\n",
      "player_team_map: {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 2, 18: 2, 19: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2}\n",
      "team_players_map: {0: [1, 2, 3, 4, 5, 6, 7, 8], 1: [9, 10, 11, 12, 13, 14, 15, 16], 2: [17, 18, 19, 20, 21, 22, 23, 24]}\n",
      "agents: [<nmmo.entity.player.Player object at 0x7e96549861f0>, <nmmo.entity.player.Player object at 0x7e9654d1d130>, <nmmo.entity.player.Player object at 0x7e9654ebd070>, <nmmo.entity.player.Player object at 0x7e9654ebdf70>, <nmmo.entity.player.Player object at 0x7e9654ea5eb0>, <nmmo.entity.player.Player object at 0x7e965517fdf0>, <nmmo.entity.player.Player object at 0x7e965516cd30>, <nmmo.entity.player.Player object at 0x7e96554f4c70>, <nmmo.entity.player.Player object at 0x7e96554c2bb0>, <nmmo.entity.player.Player object at 0x7e9655df6af0>, <nmmo.entity.player.Player object at 0x7e9655fb9a30>, <nmmo.entity.player.Player object at 0x7e9655fa2970>, <nmmo.entity.player.Player object at 0x7e96561d48b0>, <nmmo.entity.player.Player object at 0x7e96563f77f0>, <nmmo.entity.player.Player object at 0x7e96563e8730>, <nmmo.entity.player.Player object at 0x7e965667f670>, <nmmo.entity.player.Player object at 0x7e96566455b0>, <nmmo.entity.player.Player object at 0x7e96569684f0>, <nmmo.entity.player.Player object at 0x7e9656949430>, <nmmo.entity.player.Player object at 0x7e96545e4370>, <nmmo.entity.player.Player object at 0x7e96546fe2b0>, <nmmo.entity.player.Player object at 0x7e96546f01f0>, <nmmo.entity.player.Player object at 0x7e96546ca130>, <nmmo.entity.player.Player object at 0x7e965496d070>]\n",
      "_agent_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "\n",
      "Reset teams and obs\n",
      "\n",
      "Step 1\n",
      "Step observation keys: dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])\n",
      "Computing nmmo agent actions...\n",
      "COMPUTE ACTIONS CALLED\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [337]\u001b[0m, in \u001b[0;36m<cell line: 801>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    810\u001b[0m agent \u001b[38;5;241m=\u001b[39m agent_id_to_agent[agent_id]\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# actions = agent(observations[agent_id])\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# actions = agent.compute_single_action(observations[agent_id])\u001b[39;00m\n\u001b[0;32m--> 813\u001b[0m actions, state_outs, info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent ob: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# print(f'actions: {actions}')\u001b[39;00m\n",
      "Input \u001b[0;32mIn [337]\u001b[0m, in \u001b[0;36mRLlibRandom.compute_actions\u001b[0;34m(self, obs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing nmmo agent actions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnmmo agent actions computed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing random agent actions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [337]\u001b[0m, in \u001b[0;36mRLlibNMMOPolicy.compute_actions\u001b[0;34m(self, obs_batch, state_batches, prev_action_batch, prev_reward_batch, info_batch, episodes, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs_batch\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 107\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mobs_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing obs from obs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(obs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "from ray.rllib.policy.policy import Policy\n",
    "        \n",
    "# class RLlibNMMOPolicy(Policy):\n",
    "#     \"\"\"Pass NMMO agent to make compatible with RLlib envs.\"\"\"\n",
    "#     def __init__(self, \n",
    "#                  observation_space, \n",
    "#                  action_space, \n",
    "#                  config,\n",
    "#                  idx,\n",
    "#                  nmmo_agent_cls, # can be string path to cls or raw cls\n",
    "#                  **kwargs):\n",
    "#         self.observation_space = observation_space\n",
    "#         self.action_space = action_space\n",
    "#         self.config = config\n",
    "#         if isinstance(nmmo_agent_cls, str):\n",
    "#             self.nmmo_agent = get_class_from_path(nmmo_agent_cls)(config=config, idx=idx)\n",
    "#         else:\n",
    "#             self.nmmo_agent = nmmo_agent_cls(config=config, idx=idx)\n",
    "#         # self.nmmo_agent = nmmo_agent\n",
    "#         # self.action_space.seed(_seed)\n",
    "        \n",
    "        \n",
    "\n",
    "#     def compute_actions(self,\n",
    "#                         obs_batch,\n",
    "#                         state_batches=None,\n",
    "#                         prev_action_batch=None,\n",
    "#                         prev_reward_batch=None,\n",
    "#                         info_batch=None,\n",
    "#                         episodes=None,\n",
    "#                         **kwargs):\n",
    "#         \"\"\"Compute actions on a batch of observations.\"\"\"\n",
    "#         # return [self.action_space.sample() for _ in obs_batch], [], {}\n",
    "#         return [self.nmmo_agent(obs) for obs in obs_batch], [], {}\n",
    "\n",
    "#     # def compute_single_action(self, obs, *args, **kwargs):\n",
    "#     #     return self.nmmo_agent(obs)\n",
    "    \n",
    "#     def learn_on_batch(self, samples):\n",
    "#         \"\"\"No learning.\"\"\"\n",
    "#         #return {}\n",
    "#         pass\n",
    "\n",
    "#     def get_weights(self):\n",
    "#         pass\n",
    "\n",
    "#     def set_weights(self, weights):\n",
    "#         pass\n",
    "\n",
    "\n",
    "\n",
    "# class RLlibNMMOPolicy(Policy, nmmo.Agent):\n",
    "class RLlibNMMOPolicy(Policy):\n",
    "    \"\"\"Pass NMMO agent to make compatible with RLlib envs.\"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 config, # must include 'env_config' and 'idx'\n",
    "                 **kwargs):\n",
    "        Policy.__init__(self, observation_space=observation_space, action_space=action_space, config={})\n",
    "        for key in ['env_config', 'idx']:\n",
    "            if key not in config:\n",
    "                raise Exception(f'Config must include key {key}')\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.idx = config['idx']\n",
    "        self.env_config = config['env_config']\n",
    "        # if isinstance(nmmo_agent_cls, str):\n",
    "        #     self.nmmo_agent = get_class_from_path(nmmo_agent_cls)(config=config, idx=idx)\n",
    "        # else:\n",
    "        #     self.nmmo_agent = nmmo_agent_cls(config=config, idx=idx)\n",
    "        \n",
    "        self.health_max = self.env_config.PLAYER_BASE_HEALTH\n",
    "\n",
    "        if self.env_config.RESOURCE_SYSTEM_ENABLED:\n",
    "            self.food_max = self.env_config.RESOURCE_BASE\n",
    "            self.water_max = self.env_config.RESOURCE_BASE\n",
    "\n",
    "        self.spawnR = None\n",
    "        self.spawnC = None\n",
    "\n",
    "    def compute_single_action(self, *args, **kwargs):\n",
    "        print(f'COMPUTE SINGLE ACTION CALLED')\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        \"\"\"Compute actions on a batch of observations.\"\"\"\n",
    "        # return [self.action_space.sample() for _ in obs_batch], [], {}\n",
    "        # return [self.nmmo_agent(obs) for obs in obs_batch], [], {}\n",
    "        print(f'COMPUTE ACTIONS CALLED')\n",
    "        \n",
    "        # HACK: Assume not batching (since not supported by nmmo agents as far as I can see)\n",
    "        if isinstance(obs_batch, list):\n",
    "            raise Exception(f'Have not implemented observation batching, currently assumes obs_batch is just obs')\n",
    "        obs = obs_batch\n",
    "        \n",
    "        self.actions = {}\n",
    "\n",
    "        obs = obs_batch[0]\n",
    "        print(f'Computing obs from obs:\\n{obs}')\n",
    "        print(f'obs len: {len(obs)}')\n",
    "        print(f'obs[0] len: {len(obs[0])}')\n",
    "        print(f'obs shape: {np.array(obs).shape}')\n",
    "        self.ob = scripting.Observation(self.env_config, obs)\n",
    "        print(f'Computed obs!!!')\n",
    "        agent = self.ob.agent\n",
    "\n",
    "        # Time Alive\n",
    "        self.timeAlive = scripting.Observation.attribute(\n",
    "            agent, Serialized.Entity.TimeAlive)\n",
    "\n",
    "        # Pos\n",
    "        self.r = scripting.Observation.attribute(agent, Serialized.Entity.R)\n",
    "        self.c = scripting.Observation.attribute(agent, Serialized.Entity.C)\n",
    "\n",
    "        #Resources\n",
    "        self.health = scripting.Observation.attribute(agent,\n",
    "                                                      Serialized.Entity.Health)\n",
    "        self.food = scripting.Observation.attribute(agent,\n",
    "                                                    Serialized.Entity.Food)\n",
    "        self.water = scripting.Observation.attribute(agent,\n",
    "                                                     Serialized.Entity.Water)\n",
    "\n",
    "        #Skills\n",
    "        self.melee = scripting.Observation.attribute(agent,\n",
    "                                                     Serialized.Entity.Melee)\n",
    "        self.range = scripting.Observation.attribute(agent,\n",
    "                                                     Serialized.Entity.Range)\n",
    "        self.mage = scripting.Observation.attribute(agent,\n",
    "                                                    Serialized.Entity.Mage)\n",
    "        self.fishing = scripting.Observation.attribute(\n",
    "            agent, Serialized.Entity.Fishing)\n",
    "        self.herbalism = scripting.Observation.attribute(\n",
    "            agent, Serialized.Entity.Herbalism)\n",
    "        self.prospecting = scripting.Observation.attribute(\n",
    "            agent, Serialized.Entity.Prospecting)\n",
    "        self.carving = scripting.Observation.attribute(\n",
    "            agent, Serialized.Entity.Carving)\n",
    "        self.alchemy = scripting.Observation.attribute(\n",
    "            agent, Serialized.Entity.Alchemy)\n",
    "\n",
    "        #Combat level\n",
    "        # TODO: Get this from agent properties\n",
    "        self.level = max(self.melee, self.range, self.mage, self.fishing,\n",
    "                         self.herbalism, self.prospecting, self.carving,\n",
    "                         self.alchemy)\n",
    "\n",
    "        self.skills = {\n",
    "            skill.Melee: self.melee,\n",
    "            skill.Range: self.range,\n",
    "            skill.Mage: self.mage,\n",
    "            skill.Fishing: self.fishing,\n",
    "            skill.Herbalism: self.herbalism,\n",
    "            skill.Prospecting: self.prospecting,\n",
    "            skill.Carving: self.carving,\n",
    "            skill.Alchemy: self.alchemy\n",
    "        }\n",
    "\n",
    "        if self.spawnR is None:\n",
    "            self.spawnR = scripting.Observation.attribute(\n",
    "                agent, Serialized.Entity.R)\n",
    "        if self.spawnC is None:\n",
    "            self.spawnC = scripting.Observation.attribute(\n",
    "                agent, Serialized.Entity.C)\n",
    "\n",
    "        # When to run from death fog in BR configs\n",
    "        self.fog_criterion = None\n",
    "        if self.env_config.PLAYER_DEATH_FOG is not None:\n",
    "            step_per_tile = max(1, int(1 / self.env_config.PLAYER_DEATH_FOG_SPEED))\n",
    "            start_running = self.timeAlive > self.env_config.PLAYER_DEATH_FOG - step_per_tile\n",
    "            run_now = (self.timeAlive % step_per_tile == 0)\n",
    "            self.fog_criterion = start_running and run_now\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # def compute_single_action(self, obs, *args, **kwargs):\n",
    "    #     return self.nmmo_agent(obs)\n",
    "    \n",
    "    def learn_on_batch(self, samples):\n",
    "        \"\"\"No learning.\"\"\"\n",
    "        #return {}\n",
    "        pass\n",
    "\n",
    "    def get_weights(self):\n",
    "        pass\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    @property\n",
    "    def forage_criterion(self) -> bool:\n",
    "        '''Return true if low on food or water'''\n",
    "        min_level = 7 * self.env_config.RESOURCE_DEPLETION_RATE\n",
    "        return self.food <= min_level or self.water <= min_level\n",
    "\n",
    "    def forage(self):\n",
    "        '''Min/max food and water using Dijkstra's algorithm'''\n",
    "        move.forageDijkstra(self.env_config, self.ob, self.actions, self.food_max,\n",
    "                            self.water_max)\n",
    "\n",
    "    def gather(self, resource):\n",
    "        '''BFS search for a particular resource'''\n",
    "        return move.gatherBFS(self.env_config, self.ob, self.actions, resource)\n",
    "\n",
    "    def explore(self):\n",
    "        '''Route away from spawn'''\n",
    "        sz = self.env_config.MAP_SIZE\n",
    "        centR, centC = sz // 2, sz // 2\n",
    "        if self.timeAlive < sz // 4:\n",
    "            move.explore(self.env_config, self.ob, self.actions, self.spawnR,\n",
    "                         self.spawnC)\n",
    "        elif self.fog_criterion:\n",
    "            move.explore(self.env_config, self.ob, self.actions, self.r, self.c)\n",
    "        else:\n",
    "            move.explore(self.env_config, self.ob, self.actions, centR, centC)\n",
    "        #move.explore(self.env_config, self.ob, self.actions, self.spawnR, self.spawnC)\n",
    "\n",
    "    @property\n",
    "    def downtime(self):\n",
    "        '''Return true if agent is not occupied with a high-priority action'''\n",
    "        return not self.forage_criterion and self.attacker is None\n",
    "\n",
    "    def evade(self):\n",
    "        '''Target and path away from an attacker'''\n",
    "        move.evade(self.env_config, self.ob, self.actions, self.attacker)\n",
    "        self.target = self.attacker\n",
    "        self.targetID = self.attackerID\n",
    "        self.targetDist = self.attackerDist\n",
    "\n",
    "    def attack(self):\n",
    "        '''Attack the current target'''\n",
    "        if self.target is not None:\n",
    "            assert self.targetID is not None\n",
    "            style = random.choice(self.style)\n",
    "            attack.target(self.env_config, self.actions, style, self.targetID)\n",
    "\n",
    "    def target_weak(self):\n",
    "        '''Target the nearest agent if it is weak'''\n",
    "        if self.closest is None:\n",
    "            return False\n",
    "\n",
    "        selfLevel = scripting.Observation.attribute(self.ob.agent,\n",
    "                                                    Serialized.Entity.Level)\n",
    "        targLevel = scripting.Observation.attribute(self.closest,\n",
    "                                                    Serialized.Entity.Level)\n",
    "        population = scripting.Observation.attribute(\n",
    "            self.closest, Serialized.Entity.Population)\n",
    "\n",
    "        if population == -1 or targLevel <= selfLevel <= 5 or selfLevel >= targLevel + 3:\n",
    "            self.target = self.closest\n",
    "            self.targetID = self.closestID\n",
    "            self.targetDist = self.closestDist\n",
    "\n",
    "    def scan_agents(self):\n",
    "        '''Scan the nearby area for agents'''\n",
    "        self.closest, self.closestDist = attack.closestTarget(\n",
    "            self.env_config, self.ob)\n",
    "        self.attacker, self.attackerDist = attack.attacker(\n",
    "            self.env_config, self.ob)\n",
    "\n",
    "        self.closestID = None\n",
    "        if self.closest is not None:\n",
    "            self.closestID = scripting.Observation.attribute(\n",
    "                self.closest, Serialized.Entity.ID)\n",
    "\n",
    "        self.attackerID = None\n",
    "        if self.attacker is not None:\n",
    "            self.attackerID = scripting.Observation.attribute(\n",
    "                self.attacker, Serialized.Entity.ID)\n",
    "\n",
    "        self.target = None\n",
    "        self.targetID = None\n",
    "        self.targetDist = None\n",
    "\n",
    "    def adaptive_control_and_targeting(self, explore=True):\n",
    "        '''Balanced foraging, evasion, and exploration'''\n",
    "        self.scan_agents()\n",
    "\n",
    "        if self.attacker is not None:\n",
    "            self.evade()\n",
    "            return\n",
    "\n",
    "        if self.fog_criterion:\n",
    "            self.explore()\n",
    "        elif self.forage_criterion or not explore:\n",
    "            self.forage()\n",
    "        else:\n",
    "            self.explore()\n",
    "\n",
    "        self.target_weak()\n",
    "\n",
    "    def process_inventory(self):\n",
    "        if not self.env_config.ITEM_SYSTEM_ENABLED:\n",
    "            return\n",
    "\n",
    "        self.inventory = set()\n",
    "        self.best_items = {}\n",
    "        self.item_counts = defaultdict(int)\n",
    "\n",
    "        self.item_levels = {\n",
    "            item.Hat: self.level,\n",
    "            item.Top: self.level,\n",
    "            item.Bottom: self.level,\n",
    "            item.Sword: self.melee,\n",
    "            item.Bow: self.range,\n",
    "            item.Wand: self.mage,\n",
    "            item.Rod: self.fishing,\n",
    "            item.Gloves: self.herbalism,\n",
    "            item.Pickaxe: self.prospecting,\n",
    "            item.Chisel: self.carving,\n",
    "            item.Arcane: self.alchemy,\n",
    "            item.Scrap: self.melee,\n",
    "            item.Shaving: self.range,\n",
    "            item.Shard: self.mage\n",
    "        }\n",
    "\n",
    "        self.gold = scripting.Observation.attribute(self.ob.agent,\n",
    "                                                    Serialized.Entity.Gold)\n",
    "\n",
    "        for item_ary in self.ob.items:\n",
    "            itm = Item(item_ary)\n",
    "            cls = itm.cls\n",
    "\n",
    "            assert itm.cls.__name__ == 'Gold' or itm.quantity != 0\n",
    "            #if itm.quantity == 0:\n",
    "            #   continue\n",
    "\n",
    "            self.item_counts[cls] += itm.quantity\n",
    "            self.inventory.add(itm)\n",
    "\n",
    "            #Too high level to equip\n",
    "            if cls in self.item_levels and itm.level > self.item_levels[cls]:\n",
    "                continue\n",
    "\n",
    "            #Best by default\n",
    "            if cls not in self.best_items:\n",
    "                self.best_items[cls] = itm\n",
    "\n",
    "            best_itm = self.best_items[cls]\n",
    "\n",
    "            if itm.level > best_itm.level:\n",
    "                self.best_items[cls] = itm\n",
    "\n",
    "            if __debug__:\n",
    "                err = 'Key {} must be an Item object'.format(cls)\n",
    "                assert isinstance(self.best_items[cls], Item), err\n",
    "\n",
    "    def upgrade_heuristic(self, current_level, upgrade_level, price):\n",
    "        return (upgrade_level - current_level) / max(price, 1)\n",
    "\n",
    "    def process_market(self):\n",
    "        if not self.env_config.EXCHANGE_SYSTEM_ENABLED:\n",
    "            return\n",
    "\n",
    "        self.market = set()\n",
    "        self.best_heuristic = {}\n",
    "\n",
    "        for item_ary in self.ob.market:\n",
    "            itm = Item(item_ary)\n",
    "            cls = itm.cls\n",
    "\n",
    "            self.market.add(itm)\n",
    "\n",
    "            #Prune Unaffordable\n",
    "            if itm.price > self.gold:\n",
    "                continue\n",
    "\n",
    "            #Too high level to equip\n",
    "            if cls in self.item_levels and itm.level > self.item_levels[cls]:\n",
    "                continue\n",
    "\n",
    "            #Current best item level\n",
    "            current_level = 0\n",
    "            if cls in self.best_items:\n",
    "                current_level = self.best_items[cls].level\n",
    "\n",
    "            itm.heuristic = self.upgrade_heuristic(current_level, itm.level,\n",
    "                                                   itm.price)\n",
    "\n",
    "            #Always count first item\n",
    "            if cls not in self.best_heuristic:\n",
    "                self.best_heuristic[cls] = itm\n",
    "                continue\n",
    "\n",
    "            #Better heuristic value\n",
    "            if itm.heuristic > self.best_heuristic[cls].heuristic:\n",
    "                self.best_heuristic[cls] = itm\n",
    "\n",
    "    def equip(self, items: set):\n",
    "        for cls, itm in self.best_items.items():\n",
    "            if cls not in items:\n",
    "                continue\n",
    "\n",
    "            if itm.equipped:\n",
    "                continue\n",
    "\n",
    "            self.actions[Action.Use] = {Action.Item: itm.instance}\n",
    "\n",
    "            return True\n",
    "\n",
    "    def consume(self):\n",
    "        if self.health <= self.health_max // 2 and item.Poultice in self.best_items:\n",
    "            itm = self.best_items[item.Poultice]\n",
    "        elif (self.food == 0\n",
    "              or self.water == 0) and item.Ration in self.best_items:\n",
    "            itm = self.best_items[item.Ration]\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.actions[Action.Use] = {Action.Item: itm.instance}\n",
    "\n",
    "    def sell(self, keep_k: dict, keep_best: set):\n",
    "        for itm in self.inventory:\n",
    "            price = itm.level\n",
    "            cls = itm.cls\n",
    "\n",
    "            if cls == item.Gold:\n",
    "                continue\n",
    "\n",
    "            assert itm.quantity > 0\n",
    "\n",
    "            if cls in keep_k:\n",
    "                owned = self.item_counts[cls]\n",
    "                k = keep_k[cls]\n",
    "                if owned <= k:\n",
    "                    continue\n",
    "\n",
    "            #Exists an equippable of the current class, best needs to be kept, and this is the best item\n",
    "            if cls in self.best_items and cls in keep_best and itm.instance == self.best_items[\n",
    "                    cls].instance:\n",
    "                continue\n",
    "\n",
    "            self.actions[Action.Sell] = {\n",
    "                Action.Item: itm.instance,\n",
    "                Action.Price: Action.Price.edges[int(price)],\n",
    "            }\n",
    "\n",
    "            return itm\n",
    "\n",
    "    def buy(self, buy_k: dict, buy_upgrade: set):\n",
    "        if len(self.inventory) >= self.env_config.ITEM_INVENTORY_CAPACITY:\n",
    "            return\n",
    "\n",
    "        purchase = None\n",
    "        best = list(self.best_heuristic.items())\n",
    "        random.shuffle(best)\n",
    "        for cls, itm in best:\n",
    "            #Buy top k\n",
    "            if cls in buy_k:\n",
    "                owned = self.item_counts[cls]\n",
    "                k = buy_k[cls]\n",
    "                if owned < k:\n",
    "                    purchase = itm\n",
    "\n",
    "            #Check if item desired\n",
    "            if cls not in buy_upgrade:\n",
    "                continue\n",
    "\n",
    "            #Check is is an upgrade\n",
    "            if itm.heuristic <= 0:\n",
    "                continue\n",
    "\n",
    "            #Buy best heuristic upgrade\n",
    "            self.actions[Action.Buy] = {Action.Item: itm.instance}\n",
    "\n",
    "            return itm\n",
    "\n",
    "    def exchange(self):\n",
    "        if not self.env_config.EXCHANGE_SYSTEM_ENABLED:\n",
    "            return\n",
    "\n",
    "        self.process_market()\n",
    "        self.sell(keep_k=self.supplies, keep_best=self.wishlist)\n",
    "        self.buy(buy_k=self.supplies, buy_upgrade=self.wishlist)\n",
    "\n",
    "    def use(self):\n",
    "        self.process_inventory()\n",
    "        if self.env_config.EQUIPMENT_SYSTEM_ENABLED and not self.consume():\n",
    "            self.equip(items=self.wishlist)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class RLlibRandom(RLlibNMMOPolicy):\n",
    "    def __init__(self,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        RLlibNMMOPolicy.__init__(self, *args, **kwargs)\n",
    "        \n",
    "    def compute_actions(self, obs, *args, **kwargs):\n",
    "        print(f'Computing nmmo agent actions...')\n",
    "        super().compute_actions(obs)\n",
    "        print(f'nmmo agent actions computed')\n",
    "\n",
    "        print(f'Computing random agent actions...')\n",
    "        move.random(self.config, self.ob, self.actions)\n",
    "        print(f'Random agent actions computed')\n",
    "        return self.actions, [], {}\n",
    "\n",
    "\n",
    "class RLlibMultiAgentTeamBasedEnv(MultiAgentEnv):\n",
    "    def __init__(self, \n",
    "                 env_config,\n",
    "                 # path_to_env_cls,\n",
    "                 # path_to_env_config_cls,\n",
    "                 teams_config):\n",
    "        '''\n",
    "        Notes on interacting with _env TeamBasedEnv internally:\n",
    "            - TeamBasedEnv.players: Dict mapping player ID to player object\n",
    "            - TeamBasedEnv.player_team_map: Dict mapping player ID to team ID\n",
    "            - TeamBasedEnv.team_player_map: Dict mapping team ID to list of player IDs\n",
    "        '''\n",
    "        # inherit from RLlib multi-agent env\n",
    "        MultiAgentEnv.__init__(self)\n",
    "        \n",
    "        # # init env config\n",
    "        # env_config = get_class_from_path(path_to_env_config_cls)()\n",
    "        \n",
    "        # HACK: init dummy teams and env so can init RLlib policies with obs and action space\n",
    "        from neurips2022nmmo.scripted import RandomTeam\n",
    "        dummy_teams = [RandomTeam(team_id=i, env_config=env_config) for i in range(len(list(teams_config.keys())))]\n",
    "        for i, team in enumerate(dummy_teams):\n",
    "            class Agent(nmmo.Agent):\n",
    "                name = f'{team.id}'\n",
    "                policy = f'{team.id}'\n",
    "            env_config.PLAYERS[i] = Agent\n",
    "        dummy_env = get_class_from_path(path_to_env_cls)(env_config)\n",
    "        _ = dummy_env.reset() # need to call this for RLlib to make obs and action space accessible\n",
    "        \n",
    "        # use dummy env to init single agent observation and action space\n",
    "        dummy_team_id = list(dummy_env.team_players_map.keys())[0]\n",
    "        self.observation_space = dummy_env.observation_space(dummy_team_id)\n",
    "        self.action_space = dummy_env.action_space(dummy_team_id)\n",
    "        print(f'observation_space: {self.observation_space}')\n",
    "        print(f'action_space: {self.action_space}')\n",
    "\n",
    "        # init teams\n",
    "        self.teams = []\n",
    "        for team_id, params in teams_config.items():\n",
    "            team_cls, team_kwargs = params['cls'], params['kwargs']\n",
    "            team_kwargs['env_config'] = env_config\n",
    "            team_kwargs['team_id'] = team_id\n",
    "            for player_idx in range(len(team_kwargs['agents_cls'])):\n",
    "                team_kwargs['agents_kwargs'][player_idx] = {'observation_space': self.observation_space, 'action_space': self.action_space}\n",
    "            # team_kwargs['observation_space'] = self.observation_space\n",
    "            # team_kwargs['action_space'] = self.action_space\n",
    "            if 'config' not in team_kwargs:\n",
    "                team_kwargs['config'] = {}\n",
    "            self.teams.append(team_cls(**team_kwargs))\n",
    "        print(f'Teams: {self.teams}')\n",
    "\n",
    "        # update env config with instantiated teams\n",
    "        for i, team in enumerate(self.teams):\n",
    "            class Agent(nmmo.Agent):\n",
    "                name = f'{team.id}'\n",
    "                policy = f'{team.id}'\n",
    "            env_config.PLAYERS[i] = Agent\n",
    "            print(f'env_config.PLAYERS[{i}]: {Agent}')\n",
    "        print(f'env_config: {env_config}')\n",
    "                \n",
    "        # init TeamBasedEnv with env config\n",
    "        self._env = get_class_from_path(path_to_env_cls)(env_config)\n",
    "        _ = self._env.reset() # need to call this for RLlib to make obs and action space accessible\n",
    "        print(f'_env: {self._env}')\n",
    "        print(f'players: {self._env.players}')\n",
    "        print(f'player_team_map: {self._env.player_team_map}')\n",
    "        print(f'team_players_map: {self._env.team_players_map}')\n",
    "        \n",
    "        # init agent info\n",
    "        self._agent_ids = list(self._env.players.keys())\n",
    "        self.agents = list(self._env.players.values())\n",
    "        print(f'agents: {self.agents}')\n",
    "        print(f'_agent_ids: {self._agent_ids}')\n",
    "        \n",
    "        self.dones = set()\n",
    "        \n",
    "    @override(MultiAgentEnv)\n",
    "    def observation_space_sample(self, agent_ids: list = None) -> MultiAgentDict:\n",
    "        if agent_ids is None:\n",
    "            # agent_ids = list(range(len(self.agents)))\n",
    "            agent_ids = self._agent_ids\n",
    "        obs = {agent_id: self.observation_space.sample() for agent_id in agent_ids}\n",
    "\n",
    "        return obs\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def action_space_sample(self, agent_ids: list = None) -> MultiAgentDict:\n",
    "        if agent_ids is None:\n",
    "            # agent_ids = list(range(len(self.agents)))\n",
    "            agent_ids = self._agent_ids\n",
    "        actions = {agent_id: self.action_space.sample() for agent_id in agent_ids}\n",
    "\n",
    "        return actions\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def action_space_contains(self, x: MultiAgentDict) -> bool:\n",
    "        if not isinstance(x, dict):\n",
    "            return False\n",
    "        return all(self.action_space.contains(val) for val in x.values())\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def observation_space_contains(self, x: MultiAgentDict) -> bool:\n",
    "        if not isinstance(x, dict):\n",
    "            return False\n",
    "        return all(self.observation_space.contains(val) for val in x.values())\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def reset(self):\n",
    "        self.dones = set()\n",
    "        # return {i: a.reset() for i, a in enumerate(self.agents)}\n",
    "        \n",
    "        self.teams = reset_teams(self.teams)\n",
    "        \n",
    "        self.agent_id_to_agent = {}\n",
    "        for team_idx, team in enumerate(self.teams):\n",
    "            for player_idx, player_id in enumerate(self._env.team_players_map[team_idx]):\n",
    "                self.agent_id_to_agent[player_id] = team.agents[player_idx]\n",
    "                \n",
    "        obs = reset_env(self._env)\n",
    "        \n",
    "        # self.agent_id_to_agent = self._env.players\n",
    "        \n",
    "        return self._flatten_teams_dict(obs)\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def step(self, action_dict):\n",
    "        print(f'player to actions:\\n{action_dict.keys()}\\n{action_dict}')\n",
    "        \n",
    "        # convert player dict into team dict\n",
    "        team_actions = self._unflatten_players_dict(action_dict)\n",
    "        print(f'team to actions:\\n{team_actions.keys()}\\n{team_actions}')\n",
    "        \n",
    "        # # do post processing of actions\n",
    "        for team_idx, actions in team_actions.items():\n",
    "            team_actions[team_idx] = self.post_process_team_actions(self.teams[team_idx], actions)\n",
    "        \n",
    "        # step TeamBasedEnv\n",
    "        obs, rew, done, info = self._env.step(team_actions)\n",
    "        done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "        print(f'TeamBasedEnv obs keys: {obs.keys()}')\n",
    "        for key in obs.keys():\n",
    "            print(f'TeamBasedEnv obs[{key}] keys: {obs[key].keys()}')\n",
    "        \n",
    "        # convert team dicts into player dicts\n",
    "        obs, rew, done, info = [self._flatten_teams_dict(_dict) for _dict in [obs, rew, done, info]]\n",
    "        print(f'RLlibMultiAgentTeamBasedEnv obs keys: {obs.keys()}')\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "    \n",
    "    def _flatten_teams_dict(self, _dict):\n",
    "        flat_dict = {}\n",
    "        # print(f'dict to flatten: {_dict}')\n",
    "        for team_idx, player_ids in self._env.team_players_map.items():\n",
    "            if team_idx in _dict:\n",
    "                if 'stat' in _dict[team_idx]:\n",
    "                    stat = _dict[team_idx].pop('stat')\n",
    "                for player_idx in _dict[team_idx].keys():\n",
    "                    player_id = player_ids[player_idx]\n",
    "                    flat_dict[player_id] = _dict[team_idx][player_idx]\n",
    "        return flat_dict\n",
    "    \n",
    "    def _unflatten_players_dict(self, _dict):\n",
    "        unflat_dict = defaultdict(dict)\n",
    "        for player_id in _dict.keys():\n",
    "            team_idx = self._env.player_team_map[player_id]\n",
    "            player_idx = self._env.team_players_map[team_idx].index(player_id)\n",
    "            unflat_dict[team_idx][player_idx] = _dict[player_id]\n",
    "        return unflat_dict\n",
    "    \n",
    "    def post_process_team_actions(self, team, actions, verbose=True):\n",
    "        if verbose:\n",
    "            print(f'\\nteam: {team}')\n",
    "        for i in actions:\n",
    "            if verbose:\n",
    "                print(f'\\ti: {i}')\n",
    "            for atn, args in actions[i].items():\n",
    "                if verbose:\n",
    "                    print(f'atn: {atn}')\n",
    "                    print(f'args: {args}')\n",
    "                for arg, val in args.items():\n",
    "                    if verbose:\n",
    "                        print(f'arg: {arg}')\n",
    "                        print(f'val: {val}')\n",
    "                        print(f'edges: {arg.edges}')\n",
    "                    try:\n",
    "                        if arg.argType == nmmo.action.Fixed:\n",
    "                            actions[i][atn][arg] = arg.edges.index(val)\n",
    "                        elif arg == nmmo.action.Target:\n",
    "                            actions[i][atn][arg] = self.get_target_index(\n",
    "                                val, team.agents[i].ob.agents)\n",
    "                        elif atn in (nmmo.action.Sell,\n",
    "                                     nmmo.action.Use) and arg == nmmo.action.Item:\n",
    "                            actions[i][atn][arg] = self.get_item_index(\n",
    "                                val, team.agents[i].ob.items)\n",
    "                        elif atn == nmmo.action.Buy and arg == nmmo.action.Item:\n",
    "                            actions[i][atn][arg] = self.get_item_index(\n",
    "                                val, team.agents[i].ob.market)\n",
    "                    except ValueError:\n",
    "                        # TODO TEMP HACK: RLlib Epoch loop seems to return actions as ints rather than objects, somewhere under hood is this conversion done already?!\n",
    "                        # need to figure this out\n",
    "                        actions[i][atn][arg] = val\n",
    "        return actions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_item_index(instance: int, items: np.ndarray) -> int:\n",
    "        for i, itm in enumerate(items):\n",
    "            id_ = nmmo.scripting.Observation.attribute(itm,\n",
    "                                                       nmmo.Serialized.Item.ID)\n",
    "            if id_ == instance:\n",
    "                return i\n",
    "        raise ValueError(f\"Instance {instance} not found\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target_index(target: int, agents: np.ndarray) -> int:\n",
    "        targets = [\n",
    "            x for x in [\n",
    "                nmmo.scripting.Observation.attribute(\n",
    "                    agent, nmmo.Serialized.Entity.ID) for agent in agents\n",
    "            ] if x\n",
    "        ]\n",
    "        return targets.index(target)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# env config\n",
    "path_to_env_cls = 'neurips2022nmmo.TeamBasedEnv'\n",
    "path_to_env_config_cls = 'deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig'\n",
    "env_config_kwargs = None\n",
    "# teams_config = {\n",
    "#     'Combat':\n",
    "#         {\n",
    "#             'path_to_team_cls': 'neurips2022nmmo.scripted.CombatTeam'\n",
    "#         },\n",
    "#     'Mixture':\n",
    "#         {\n",
    "#             'path_to_team_cls': 'neurips2022nmmo.scripted.MixtureTeam'\n",
    "#         },\n",
    "# }\n",
    "# rllib_paths_to_scripted_agents_cls = {\n",
    "#                                     '0': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     '1': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     '2': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     '3': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     '4': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     '5': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     '6': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "#                                     }\n",
    "\n",
    "num_players_per_team = 8\n",
    "# will overwrite agents kwargs with observation and action space and player index when initialising teams in RLlib env\n",
    "custom_team_kwargs = {'agents_cls': [RLlibRandom for _ in range(num_players_per_team)], 'agents_kwargs': [None for _ in range(num_players_per_team)]}\n",
    "\n",
    "teams_config = {\n",
    "    # 'RLlib': {'cls': RLlibScriptedHybridAgentTeam, 'kwargs': {'paths_to_scripted_agents_cls': rllib_paths_to_scripted_agents_cls}},\n",
    "    \n",
    "    # 'Combat-1': {'cls': neurips2022nmmo.scripted.CombatTeam, 'kwargs': {'team_id': 1}},\n",
    "    # 'Combat-2': {'cls': neurips2022nmmo.scripted.CombatTeam, 'kwargs': {'team_id': 2}},\n",
    "    # 'Mixture-1': {'cls': neurips2022nmmo.scripted.MixtureTeam, 'kwargs': {'team_id': 3}},\n",
    "    \n",
    "    'R-1': {'cls': deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam, 'kwargs': custom_team_kwargs},\n",
    "    'R-2': {'cls': deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam, 'kwargs': custom_team_kwargs},\n",
    "    'R-3': {'cls': deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam, 'kwargs': custom_team_kwargs},\n",
    "    \n",
    "    \n",
    "}\n",
    "# teams_copies = [1, 2]        \n",
    "        \n",
    "\n",
    "env_config = get_class_from_path(path_to_env_config_cls)()\n",
    "ma_env = RLlibMultiAgentTeamBasedEnv(env_config=env_config,\n",
    "                                     # path_to_env_cls=path_to_env_cls,\n",
    "                                     # path_to_env_config_cls=path_to_env_config_cls,\n",
    "                                     teams_config=teams_config)\n",
    "observations = ma_env.reset()\n",
    "teams, agent_id_to_agent = ma_env.teams, ma_env.agent_id_to_agent\n",
    "print(f'')\n",
    "# print(f'RESET\\nobs:\\n{obs}\\nteams:\\n{teams}')\n",
    "print(f'Reset teams and obs')\n",
    "step_counter = 1\n",
    "while observations:\n",
    "    print(f'\\nStep {step_counter}')\n",
    "    print(f'Step observation keys: {observations.keys()}')\n",
    "    # agent_to_actions = {agent_id: agent(observations[agent_id]) for agent_id, agent in agent_id_to_agent.items()}\n",
    "    \n",
    "    agent_to_actions = {}\n",
    "    for agent_id in observations.keys():\n",
    "        # print(f'\\nagent_id: {agent_id}')\n",
    "        # print(f'agent: {agent}')\n",
    "        agent = agent_id_to_agent[agent_id]\n",
    "        # actions = agent(observations[agent_id])\n",
    "        # actions = agent.compute_single_action(observations[agent_id])\n",
    "        actions, state_outs, info = agent.compute_actions(observations[agent_id])\n",
    "        print(f'agent ob: {agent.ob}')\n",
    "        # print(f'actions: {actions}')\n",
    "        agent_to_actions[agent_id] = actions\n",
    "    \n",
    "    observations, rewards, dones, infos = ma_env.step(agent_to_actions)\n",
    "    raise Exception(f'Completed step.')\n",
    "    step_counter += 1\n",
    "    \n",
    "print(f'\\nEPISODE COMPLETED!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8e335-530e-44a8-ad62-8162d5f7a86c",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Get `RLlibMultiAgentTeamBasedEnv` working with `ray` and `rllib`\n",
    "    - Start by trying to use non-learning policies i.e. just map to scripted agents\n",
    "        - PROBLEM: env seems to be empty of players and teams after initialisation when run with RLlib. Why?\n",
    "- Use `RLlibScriptedHybridAgentTeam` to try and put a learning agent into the environment and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2d812990-43db-4052-b944-de070346b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_rllib_trainer_cls = 'ray.rllib.agents.ppo.PPOTrainer'\n",
    "\n",
    "env_config = get_class_from_path(path_to_env_config_cls)()\n",
    "\n",
    "policies = {}\n",
    "policy_id = 1\n",
    "num_teams, num_players_per_team = 3, 8\n",
    "for _ in range(num_teams):\n",
    "    player_idx = 0\n",
    "    for _ in range(num_players_per_team):\n",
    "        policies[str(policy_id)] = PolicySpec(\n",
    "                                    policy_class=RLlibRandom, # infer from env\n",
    "                                    # observation_space=dummy_env.observation_space(player_id),\n",
    "                                    # action_space=dummy_env.action_space(player_id),\n",
    "                                    config={'env_config': env_config, 'idx': player_idx},\n",
    "                                )\n",
    "        policy_id += 1\n",
    "        player_idx += 1\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    '''Maps agent ID (int) to corresponding policy ID (str) of policy which agent should use.'''\n",
    "    print(f'>>> Mapping agent_id {agent_id} to {str(agent_id)}')\n",
    "    return str(agent_id)\n",
    "\n",
    "multiagent_config = {\n",
    "    'policies': policies,\n",
    "    'policy_mapping_fn': policy_mapping_fn,\n",
    "    'policies_to_train': [],\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    # 'path_to_env_cls': path_to_env_cls,\n",
    "    # 'path_to_env_config_cls': path_to_env_config_cls,\n",
    "    'env_config': env_config,\n",
    "    'teams_config': teams_config,\n",
    "}\n",
    "\n",
    "\n",
    "rllib_config = {\n",
    "    'env': 'ma_env',\n",
    "    \n",
    "    'env_config': env_config,\n",
    "    \n",
    "    'disable_env_checking': True,\n",
    "    # 'disable_env_checking': False,\n",
    "    \n",
    "    'framework': 'torch',\n",
    "    \n",
    "    'multiagent': multiagent_config,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4040a863-2670-41b5-b0ce-f84c2e8133f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register env with ray\n",
    "register_env('ma_env', lambda env_config: RLlibMultiAgentTeamBasedEnv(**env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "19d5548c-6291-4692-9caa-b1363a962487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 22:03:13,032\tWARNING multi_agent.py:121 -- `config.multiagent.policies_to_train` is empty! Make sure - if you would like to learn at least one policy - to add its ID to that list.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'ma_env', 'env_config': {'env_config': <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7e965db2e160>, 'teams_config': {'R-1': {'cls': <class 'deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam'>, 'kwargs': {'agents_cls': [<class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>], 'agents_kwargs': [{'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}], 'env_config': <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7e965db10280>, 'team_id': 'R-3', 'config': {}}}, 'R-2': {'cls': <class 'deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam'>, 'kwargs': {'agents_cls': [<class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>], 'agents_kwargs': [{'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}], 'env_config': <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7e965db10280>, 'team_id': 'R-3', 'config': {}}}, 'R-3': {'cls': <class 'deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam'>, 'kwargs': {'agents_cls': [<class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>, <class '__main__.RLlibRandom'>], 'agents_kwargs': [{'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}, {'observation_space': Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32))), 'action_space': Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))}], 'env_config': <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7e965db10280>, 'team_id': 'R-3', 'config': {}}}}}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'1': <ray.rllib.policy.policy.PolicySpec object at 0x7e965db2e0d0>, '2': <ray.rllib.policy.policy.PolicySpec object at 0x7e965db2e040>, '3': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217730>, '4': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217e80>, '5': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217f10>, '6': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217910>, '7': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217b50>, '8': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217a90>, '9': <ray.rllib.policy.policy.PolicySpec object at 0x7e965db2e100>, '10': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217a00>, '11': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217a60>, '12': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217d90>, '13': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217d00>, '14': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217ca0>, '15': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217700>, '16': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217f40>, '17': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217af0>, '18': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217790>, '19': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217df0>, '20': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217ee0>, '21': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217fa0>, '22': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217dc0>, '23': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217c70>, '24': <ray.rllib.policy.policy.PolicySpec object at 0x7e965d217c10>}, 'policy_mapping_fn': <function policy_mapping_fn at 0x7e96542c33a0>, 'policies_to_train': []}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]m \n",
      "  0%|          | 0/40 [00:00<?, ?it/s]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m Generating 40 maps\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:00<00:02, 14.36it/s]\n",
      "  5%|▌         | 2/40 [00:00<00:02, 14.30it/s]\n",
      " 10%|█         | 4/40 [00:00<00:02, 14.54it/s]\n",
      " 10%|█         | 4/40 [00:00<00:02, 14.45it/s]\n",
      " 15%|█▌        | 6/40 [00:00<00:02, 14.89it/s]\n",
      " 15%|█▌        | 6/40 [00:00<00:02, 14.78it/s]\n",
      " 20%|██        | 8/40 [00:00<00:02, 15.25it/s]\n",
      " 20%|██        | 8/40 [00:00<00:02, 15.11it/s]\n",
      " 25%|██▌       | 10/40 [00:00<00:01, 15.34it/s]\n",
      " 25%|██▌       | 10/40 [00:00<00:01, 15.27it/s]\n",
      " 30%|███       | 12/40 [00:00<00:01, 15.28it/s]\n",
      " 30%|███       | 12/40 [00:00<00:01, 15.22it/s]\n",
      " 35%|███▌      | 14/40 [00:00<00:01, 15.41it/s]\n",
      " 35%|███▌      | 14/40 [00:00<00:01, 15.37it/s]\n",
      " 40%|████      | 16/40 [00:01<00:01, 15.19it/s]\n",
      " 40%|████      | 16/40 [00:01<00:01, 14.92it/s]\n",
      " 45%|████▌     | 18/40 [00:01<00:01, 14.97it/s]\n",
      " 45%|████▌     | 18/40 [00:01<00:01, 14.80it/s]\n",
      " 50%|█████     | 20/40 [00:01<00:01, 14.88it/s]\n",
      " 50%|█████     | 20/40 [00:01<00:01, 14.77it/s]\n",
      " 55%|█████▌    | 22/40 [00:01<00:01, 14.78it/s]\n",
      " 55%|█████▌    | 22/40 [00:01<00:01, 14.69it/s]\n",
      " 60%|██████    | 24/40 [00:01<00:01, 14.63it/s]\n",
      " 60%|██████    | 24/40 [00:01<00:01, 14.60it/s]\n",
      " 65%|██████▌   | 26/40 [00:01<00:00, 14.69it/s]\n",
      " 65%|██████▌   | 26/40 [00:01<00:00, 14.66it/s]\n",
      " 70%|███████   | 28/40 [00:01<00:00, 14.64it/s]\n",
      " 70%|███████   | 28/40 [00:01<00:00, 14.62it/s]\n",
      " 75%|███████▌  | 30/40 [00:02<00:00, 14.63it/s]\n",
      " 75%|███████▌  | 30/40 [00:02<00:00, 14.60it/s]\n",
      " 80%|████████  | 32/40 [00:02<00:00, 14.55it/s]\n",
      " 80%|████████  | 32/40 [00:02<00:00, 14.54it/s]\n",
      " 85%|████████▌ | 34/40 [00:02<00:00, 14.53it/s]\n",
      " 85%|████████▌ | 34/40 [00:02<00:00, 14.53it/s]\n",
      " 90%|█████████ | 36/40 [00:02<00:00, 14.60it/s]\n",
      " 90%|█████████ | 36/40 [00:02<00:00, 14.60it/s]\n",
      " 95%|█████████▌| 38/40 [00:02<00:00, 14.73it/s]\n",
      " 95%|█████████▌| 38/40 [00:02<00:00, 14.70it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 14.78it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m observation_space: Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32)))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m action_space: Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m Teams: [<deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7ef2bf494670>, <deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7ef2bf494640>, <deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7ef2c64240a0>]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m env_config.PLAYERS[0]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m env_config.PLAYERS[1]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m env_config.PLAYERS[2]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m env_config: <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7ef2c6523730>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]m \n",
      "  5%|▌         | 2/40 [00:00<00:02, 14.84it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m observation_space: Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32)))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m action_space: Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m Teams: [<deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7fca4efa8ac0>, <deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7fca4efa8a90>, <deep_nmmo.envs.team_based_env.teams.custom_team.CustomTeam object at 0x7fca55f26070>]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m env_config.PLAYERS[0]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m env_config.PLAYERS[1]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m env_config.PLAYERS[2]: <class '__main__.RLlibMultiAgentTeamBasedEnv.__init__.<locals>.Agent'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m env_config: <deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig object at 0x7fca56026fd0>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:00<00:02, 14.64it/s]\n",
      "  5%|▌         | 2/40 [00:00<00:02, 14.61it/s]\n",
      " 15%|█▌        | 6/40 [00:00<00:02, 15.37it/s]\n",
      " 10%|█         | 4/40 [00:00<00:02, 14.63it/s]\n",
      " 20%|██        | 8/40 [00:00<00:02, 15.55it/s]\n",
      " 15%|█▌        | 6/40 [00:00<00:02, 15.07it/s]\n",
      " 25%|██▌       | 10/40 [00:00<00:01, 15.78it/s]\n",
      " 20%|██        | 8/40 [00:00<00:02, 15.36it/s]\n",
      " 30%|███       | 12/40 [00:00<00:01, 15.90it/s]\n",
      " 25%|██▌       | 10/40 [00:00<00:01, 15.16it/s]\n",
      " 35%|███▌      | 14/40 [00:00<00:01, 15.91it/s]\n",
      " 30%|███       | 12/40 [00:00<00:01, 15.10it/s]\n",
      " 40%|████      | 16/40 [00:01<00:01, 15.85it/s]\n",
      " 35%|███▌      | 14/40 [00:00<00:01, 15.31it/s]\n",
      " 45%|████▌     | 18/40 [00:01<00:01, 15.95it/s]\n",
      " 40%|████      | 16/40 [00:01<00:01, 15.45it/s]\n",
      " 50%|█████     | 20/40 [00:01<00:01, 15.95it/s]\n",
      " 45%|████▌     | 18/40 [00:01<00:01, 15.57it/s]\n",
      " 55%|█████▌    | 22/40 [00:01<00:01, 15.86it/s]\n",
      " 50%|█████     | 20/40 [00:01<00:01, 15.64it/s]\n",
      " 60%|██████    | 24/40 [00:01<00:01, 15.91it/s]\n",
      " 55%|█████▌    | 22/40 [00:01<00:01, 15.69it/s]\n",
      " 65%|██████▌   | 26/40 [00:01<00:00, 15.92it/s]\n",
      " 60%|██████    | 24/40 [00:01<00:01, 15.64it/s]\n",
      " 70%|███████   | 28/40 [00:01<00:00, 16.00it/s]\n",
      " 65%|██████▌   | 26/40 [00:01<00:00, 15.51it/s]\n",
      " 75%|███████▌  | 30/40 [00:01<00:00, 15.93it/s]\n",
      " 70%|███████   | 28/40 [00:01<00:00, 15.42it/s]\n",
      " 80%|████████  | 32/40 [00:02<00:00, 15.97it/s]\n",
      " 75%|███████▌  | 30/40 [00:01<00:00, 15.20it/s]\n",
      " 85%|████████▌ | 34/40 [00:02<00:00, 15.95it/s]\n",
      " 80%|████████  | 32/40 [00:02<00:00, 15.19it/s]\n",
      " 90%|█████████ | 36/40 [00:02<00:00, 15.65it/s]\n",
      " 85%|████████▌ | 34/40 [00:02<00:00, 15.34it/s]\n",
      " 95%|█████████▌| 38/40 [00:02<00:00, 15.62it/s]\n",
      " 90%|█████████ | 36/40 [00:02<00:00, 15.38it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 15.75it/s]\n",
      " 95%|█████████▌| 38/40 [00:02<00:00, 15.50it/s]\n",
      "100%|██████████| 40/40 [00:02<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m _env: <neurips2022nmmo.env.team_based_env.TeamBasedEnv object at 0x7ef2c64172e0>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m players: {1: <nmmo.entity.player.Player object at 0x7ef2bcfb4550>, 2: <nmmo.entity.player.Player object at 0x7ef2bda0d490>, 3: <nmmo.entity.player.Player object at 0x7ef2bcfdc3d0>, 4: <nmmo.entity.player.Player object at 0x7ef2bdeb8310>, 5: <nmmo.entity.player.Player object at 0x7ef2bdea5250>, 6: <nmmo.entity.player.Player object at 0x7ef2be175190>, 7: <nmmo.entity.player.Player object at 0x7ef2be1660d0>, 8: <nmmo.entity.player.Player object at 0x7ef2be166fd0>, 9: <nmmo.entity.player.Player object at 0x7ef2be1f5f10>, 10: <nmmo.entity.player.Player object at 0x7ef2be37ee50>, 11: <nmmo.entity.player.Player object at 0x7ef2be36bd90>, 12: <nmmo.entity.player.Player object at 0x7ef2be5bfcd0>, 13: <nmmo.entity.player.Player object at 0x7ef2be5abc10>, 14: <nmmo.entity.player.Player object at 0x7ef2be636b50>, 15: <nmmo.entity.player.Player object at 0x7ef2be600a90>, 16: <nmmo.entity.player.Player object at 0x7ef2bed189d0>, 17: <nmmo.entity.player.Player object at 0x7ef2bd07f910>, 18: <nmmo.entity.player.Player object at 0x7ef2bd071850>, 19: <nmmo.entity.player.Player object at 0x7ef2bd04e790>, 20: <nmmo.entity.player.Player object at 0x7ef2bd2666d0>, 21: <nmmo.entity.player.Player object at 0x7ef2bd244610>, 22: <nmmo.entity.player.Player object at 0x7ef2bd82a550>, 23: <nmmo.entity.player.Player object at 0x7ef2bdcff490>, 24: <nmmo.entity.player.Player object at 0x7ef2bdceb3d0>}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m player_team_map: {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 2, 18: 2, 19: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m team_players_map: {0: [1, 2, 3, 4, 5, 6, 7, 8], 1: [9, 10, 11, 12, 13, 14, 15, 16], 2: [17, 18, 19, 20, 21, 22, 23, 24]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m agents: [<nmmo.entity.player.Player object at 0x7ef2bcfb4550>, <nmmo.entity.player.Player object at 0x7ef2bda0d490>, <nmmo.entity.player.Player object at 0x7ef2bcfdc3d0>, <nmmo.entity.player.Player object at 0x7ef2bdeb8310>, <nmmo.entity.player.Player object at 0x7ef2bdea5250>, <nmmo.entity.player.Player object at 0x7ef2be175190>, <nmmo.entity.player.Player object at 0x7ef2be1660d0>, <nmmo.entity.player.Player object at 0x7ef2be166fd0>, <nmmo.entity.player.Player object at 0x7ef2be1f5f10>, <nmmo.entity.player.Player object at 0x7ef2be37ee50>, <nmmo.entity.player.Player object at 0x7ef2be36bd90>, <nmmo.entity.player.Player object at 0x7ef2be5bfcd0>, <nmmo.entity.player.Player object at 0x7ef2be5abc10>, <nmmo.entity.player.Player object at 0x7ef2be636b50>, <nmmo.entity.player.Player object at 0x7ef2be600a90>, <nmmo.entity.player.Player object at 0x7ef2bed189d0>, <nmmo.entity.player.Player object at 0x7ef2bd07f910>, <nmmo.entity.player.Player object at 0x7ef2bd071850>, <nmmo.entity.player.Player object at 0x7ef2bd04e790>, <nmmo.entity.player.Player object at 0x7ef2bd2666d0>, <nmmo.entity.player.Player object at 0x7ef2bd244610>, <nmmo.entity.player.Player object at 0x7ef2bd82a550>, <nmmo.entity.player.Player object at 0x7ef2bdcff490>, <nmmo.entity.player.Player object at 0x7ef2bdceb3d0>]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m _agent_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m _env: <neurips2022nmmo.env.team_based_env.TeamBasedEnv object at 0x7fca55f192b0>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m players: {1: <nmmo.entity.player.Player object at 0x7fca4caf59a0>, 2: <nmmo.entity.player.Player object at 0x7fca4ce37910>, 3: <nmmo.entity.player.Player object at 0x7fca4ce15850>, 4: <nmmo.entity.player.Player object at 0x7fca4d22d790>, 5: <nmmo.entity.player.Player object at 0x7fca4d20c6d0>, 6: <nmmo.entity.player.Player object at 0x7fca4d432610>, 7: <nmmo.entity.player.Player object at 0x7fca4d401550>, 8: <nmmo.entity.player.Player object at 0x7fca4d6ef490>, 9: <nmmo.entity.player.Player object at 0x7fca4d6d63d0>, 10: <nmmo.entity.player.Player object at 0x7fca4d767310>, 11: <nmmo.entity.player.Player object at 0x7fca4d7f1250>, 12: <nmmo.entity.player.Player object at 0x7fca4db7c190>, 13: <nmmo.entity.player.Player object at 0x7fca4db730d0>, 14: <nmmo.entity.player.Player object at 0x7fca4db73fd0>, 15: <nmmo.entity.player.Player object at 0x7fca4db47f10>, 16: <nmmo.entity.player.Player object at 0x7fca4df2ee50>, 17: <nmmo.entity.player.Player object at 0x7fca4dfbdd90>, 18: <nmmo.entity.player.Player object at 0x7fca4dfaecd0>, 19: <nmmo.entity.player.Player object at 0x7fca4e03fc10>, 20: <nmmo.entity.player.Player object at 0x7fca4e00db50>, 21: <nmmo.entity.player.Player object at 0x7fca4e2afa90>, 22: <nmmo.entity.player.Player object at 0x7fca4e73a9d0>, 23: <nmmo.entity.player.Player object at 0x7fca4e72c910>, 24: <nmmo.entity.player.Player object at 0x7fca4e9bb850>}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m player_team_map: {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 2, 18: 2, 19: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m team_players_map: {0: [1, 2, 3, 4, 5, 6, 7, 8], 1: [9, 10, 11, 12, 13, 14, 15, 16], 2: [17, 18, 19, 20, 21, 22, 23, 24]}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m agents: [<nmmo.entity.player.Player object at 0x7fca4caf59a0>, <nmmo.entity.player.Player object at 0x7fca4ce37910>, <nmmo.entity.player.Player object at 0x7fca4ce15850>, <nmmo.entity.player.Player object at 0x7fca4d22d790>, <nmmo.entity.player.Player object at 0x7fca4d20c6d0>, <nmmo.entity.player.Player object at 0x7fca4d432610>, <nmmo.entity.player.Player object at 0x7fca4d401550>, <nmmo.entity.player.Player object at 0x7fca4d6ef490>, <nmmo.entity.player.Player object at 0x7fca4d6d63d0>, <nmmo.entity.player.Player object at 0x7fca4d767310>, <nmmo.entity.player.Player object at 0x7fca4d7f1250>, <nmmo.entity.player.Player object at 0x7fca4db7c190>, <nmmo.entity.player.Player object at 0x7fca4db730d0>, <nmmo.entity.player.Player object at 0x7fca4db73fd0>, <nmmo.entity.player.Player object at 0x7fca4db47f10>, <nmmo.entity.player.Player object at 0x7fca4df2ee50>, <nmmo.entity.player.Player object at 0x7fca4dfbdd90>, <nmmo.entity.player.Player object at 0x7fca4dfaecd0>, <nmmo.entity.player.Player object at 0x7fca4e03fc10>, <nmmo.entity.player.Player object at 0x7fca4e00db50>, <nmmo.entity.player.Player object at 0x7fca4e2afa90>, <nmmo.entity.player.Player object at 0x7fca4e73a9d0>, <nmmo.entity.player.Player object at 0x7fca4e72c910>, <nmmo.entity.player.Player object at 0x7fca4e9bb850>]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m _agent_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 22:03:24,814\tINFO trainable.py:160 -- Trainable.setup took 11.785 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-09-04 22:03:24,815\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised trainer\n"
     ]
    }
   ],
   "source": [
    "# merge rllibg trainer's default config with specified config\n",
    "path_to_agent = '.'.join(path_to_rllib_trainer_cls.split('.')[:-1])\n",
    "_rllib_config = get_module_from_path(path_to_agent).DEFAULT_CONFIG.copy()\n",
    "_rllib_config.update(rllib_config)\n",
    "print(_rllib_config)\n",
    "\n",
    "# init rllib trainer\n",
    "trainer = get_class_from_path(path_to_rllib_trainer_cls)(config=_rllib_config)\n",
    "print(f'Initialised trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8fbf91e4-027b-403d-b2bf-b267f0611fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 22:03:25,046\tWARNING algorithm.py:2178 -- Worker crashed during training or evaluation! To try to continue without failed worker(s), set `ignore_worker_failures=True`. To try to recover the failed worker(s), set `recreate_failed_workers=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 1 to 1\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=477525, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fca56f06b80>)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 806, in sample\n    batches = [self.input_reader.next()]\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n    batches = [self.get_data()]\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 282, in get_data\n    item = next(self._env_runner)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 707, in _env_runner\n    eval_results = _do_policy_eval(\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1207, in _do_policy_eval\n    eval_results[policy_id] = policy.compute_actions_from_input_dict(\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 397, in compute_actions_from_input_dict\n    return self.compute_actions(\n  File \"/tmp/ipykernel_62216/660948947.py\", line 512, in compute_actions\n  File \"/tmp/ipykernel_62216/660948947.py\", line 110, in compute_actions\nTypeError: object of type 'numpy.float32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [341]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# perform one training epoch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m     (\n\u001b[1;32m    654\u001b[0m         results,\n\u001b[1;32m    655\u001b[0m         train_iter_ctx,\n\u001b[1;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2378\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[38;5;66;03m# In case of any failures, try to ignore/recover the failed workers.\u001b[39;00m\n\u001b[1;32m   2377\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2378\u001b[0m             num_recreated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_recover_from_step_attempt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m                \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m                \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m                \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_worker_failures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrecreate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecreate_failed_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2384\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_recreated_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_recreated\n\u001b[1;32m   2386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2185\u001b[0m, in \u001b[0;36mAlgorithm.try_recover_from_step_attempt\u001b[0;34m(self, error, worker_set, ignore, recreate)\u001b[0m\n\u001b[1;32m   2176\u001b[0m     \u001b[38;5;66;03m# Error out.\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2178\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   2179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker crashed during training or evaluation! \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2180\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo try to continue without failed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`recreate_failed_workers=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2184\u001b[0m         )\n\u001b[0;32m-> 2185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;66;03m# Any other exception.\u001b[39;00m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    404\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py:100\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     97\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39msample()]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Update our counters for the stopping criterion of the while loop.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sample_batches:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/_private/worker.py:2275\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=477525, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fca56f06b80>)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 806, in sample\n    batches = [self.input_reader.next()]\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n    batches = [self.get_data()]\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 282, in get_data\n    item = next(self._env_runner)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 707, in _env_runner\n    eval_results = _do_policy_eval(\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1207, in _do_policy_eval\n    eval_results[policy_id] = policy.compute_actions_from_input_dict(\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 397, in compute_actions_from_input_dict\n    return self.compute_actions(\n  File \"/tmp/ipykernel_62216/660948947.py\", line 512, in compute_actions\n  File \"/tmp/ipykernel_62216/660948947.py\", line 110, in compute_actions\nTypeError: object of type 'numpy.float32' has no len()"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 2 to 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 3 to 3\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 4 to 4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 5 to 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 6 to 6\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 7 to 7\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 8 to 8\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 9 to 9\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 10 to 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 11 to 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 12 to 12\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 13 to 13\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 14 to 14\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 15 to 15\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 16 to 16\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 17 to 17\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 18 to 18\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 19 to 19\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 20 to 20\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 21 to 21\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 22 to 22\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 23 to 23\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m >>> Mapping agent_id 24 to 24\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m Computing nmmo agent actions...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m COMPUTE ACTIONS CALLED\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m Computing obs from obs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m [  1.   1.   0. ...  40. 277.  15.]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477524)\u001b[0m obs len: 10939\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 1 to 1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 2 to 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 3 to 3\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 4 to 4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 5 to 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 6 to 6\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 7 to 7\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 8 to 8\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 9 to 9\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 10 to 10\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 11 to 11\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 12 to 12\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 13 to 13\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 14 to 14\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 15 to 15\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 16 to 16\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 17 to 17\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 18 to 18\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 19 to 19\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 20 to 20\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 21 to 21\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 22 to 22\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 23 to 23\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m >>> Mapping agent_id 24 to 24\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m Computing nmmo agent actions...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m COMPUTE ACTIONS CALLED\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m Computing obs from obs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m [  1.   1.   0. ...  40. 277.  15.]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=477525)\u001b[0m obs len: 10939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 22:03:30,283\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=477524, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ef2c7404bb0>)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 806, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 282, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 707, in _env_runner\n",
      "    eval_results = _do_policy_eval(\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1207, in _do_policy_eval\n",
      "    eval_results[policy_id] = policy.compute_actions_from_input_dict(\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 397, in compute_actions_from_input_dict\n",
      "    return self.compute_actions(\n",
      "  File \"/tmp/ipykernel_62216/660948947.py\", line 512, in compute_actions\n",
      "  File \"/tmp/ipykernel_62216/660948947.py\", line 110, in compute_actions\n",
      "TypeError: object of type 'numpy.float32' has no len()\n"
     ]
    }
   ],
   "source": [
    "# perform one training epoch\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086054b9-22e2-4702-9bf0-a25eb5726036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b4ad7-222f-4ce2-8a6f-95e068ce7b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf358d-a73a-44c2-9d59-dd6fa743e857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd684627-8389-4aac-a0c6-82b7a137a392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5f45115-62f2-4030-a934-2becaa3ad000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team_id: RLlib | kwargs: {'paths_to_scripted_agents_cls': {'0': 'neurips2022nmmo.scripted.baselines.Mage', '1': 'neurips2022nmmo.scripted.baselines.Mage', '2': 'neurips2022nmmo.scripted.baselines.Mage', '3': 'neurips2022nmmo.scripted.baselines.Mage', '4': 'neurips2022nmmo.scripted.baselines.Mage', '5': 'neurips2022nmmo.scripted.baselines.Mage', '6': 'neurips2022nmmo.scripted.baselines.Mage'}}\n",
      "team_id: RLlib\n",
      "scripted_agents: [<neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc610>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2e0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2b0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc3a0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc310>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc400>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc190>]\n",
      "scripted_agent_ids: {0, 1, 2, 3, 4, 5, 6}\n",
      "scripted_agent_idxs: {0, 1, 2, 3, 4, 5, 6}\n",
      "rllib_agents: [None]\n",
      "rllib_agent_ids: {7}\n",
      "rllib_agent_idxs: {8}\n",
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space: Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32)))\n",
      "action_space: Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))\n",
      "agents: [<neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc610>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2e0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2b0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc3a0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc310>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc400>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc190>, None]\n",
      "_agent_ids: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "team_env: <RLlibMultiAgentTeamEnv instance>\n",
      "team_id: Combat | kwargs: {}\n",
      "team_id: Mixture | kwargs: {}\n",
      "Teams: [<__main__.RLlibScriptedHybridAgentTeam object at 0x7f252bfcc460>, <neurips2022nmmo.scripted.scripted_team.CombatTeam object at 0x7f2521e11fd0>, <neurips2022nmmo.scripted.scripted_team.MixtureTeam object at 0x7f2521e162b0>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class RLlibScriptedHybridAgentTeam(Team):\n",
    "    def __init__(self,\n",
    "                 team_id,\n",
    "                 env_config,\n",
    "                 paths_to_scripted_agents_cls,\n",
    "                 **kwargs):\n",
    "        super().__init__(team_id, env_config)\n",
    "        self.id = team_id\n",
    "        print(f'team_id: {self.id}')\n",
    "        \n",
    "        self.scripted_agents = [get_class_from_path(path_to_scripted_agent_cls)(config=env_config, idx=int(idx)) for idx, path_to_scripted_agent_cls in paths_to_scripted_agents_cls.items()]\n",
    "        self.scripted_agent_ids = set([int(idx) for idx in paths_to_scripted_agents_cls.keys()])\n",
    "        self.scripted_agent_idxs = set([idx for idx in range(len(self.scripted_agents))])\n",
    "        \n",
    "        self.rllib_agents = [None]\n",
    "        self.rllib_agent_ids = set([sorted(self.scripted_agent_ids)[-1]+1])\n",
    "        self.rllib_agent_idxs = set([len(self.scripted_agents)+idx for idx in range(1, len(self.rllib_agents)+1)])\n",
    "        \n",
    "        print(f'scripted_agents: {self.scripted_agents}')\n",
    "        print(f'scripted_agent_ids: {self.scripted_agent_ids}')\n",
    "        print(f'scripted_agent_idxs: {self.scripted_agent_idxs}')\n",
    "        \n",
    "        print(f'rllib_agents: {self.rllib_agents}')\n",
    "        print(f'rllib_agent_ids: {self.rllib_agent_ids}')\n",
    "        print(f'rllib_agent_idxs: {self.rllib_agent_idxs}')\n",
    "        \n",
    "        # init multi-agent env for team to interact with\n",
    "        agent_ids_to_agents = {}\n",
    "        for agent_id, agent in zip(self.scripted_agent_ids, self.scripted_agents):\n",
    "            agent_ids_to_agents[agent_id] = agent\n",
    "        for agent_id, agent in zip(self.rllib_agent_ids, self.rllib_agents):\n",
    "            agent_ids_to_agents[agent_id] = agent\n",
    "        self.team_env = RLlibMultiAgentTeamEnv(\n",
    "                                            env_config=env_config,\n",
    "                                            agent_ids_to_agents=agent_ids_to_agents,\n",
    "                                            team_id=self.id,\n",
    "                                        )\n",
    "        print(f'team_env: {self.team_env}')\n",
    "        \n",
    "    def reset(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def get_rllib_agent_actions(self, observations):\n",
    "        return {i: self.agents[i](obs) for i, obs in observations.items() if i in self.rllib_agent_idxs}\n",
    "    \n",
    "    def get_scripted_agent_actions(self, observations):\n",
    "        return {i: self.agents[i](obs) for i, obs in observations.items() if i in self.scripted_agent_idxs}\n",
    "    \n",
    "    def post_process_actions(self, actions):\n",
    "        for i in actions:\n",
    "            for atn, args in actions[i].items():\n",
    "                for arg, val in args.items():\n",
    "                    if arg.argType == nmmo.action.Fixed:\n",
    "                        actions[i][atn][arg] = arg.edges.index(val)\n",
    "                    elif arg == nmmo.action.Target:\n",
    "                        actions[i][atn][arg] = self.get_target_index(\n",
    "                            val, self.agents[i].ob.agents)\n",
    "                    elif atn in (nmmo.action.Sell,\n",
    "                                 nmmo.action.Use) and arg == nmmo.action.Item:\n",
    "                        actions[i][atn][arg] = self.get_item_index(\n",
    "                            val, self.agents[i].ob.items)\n",
    "                    elif atn == nmmo.action.Buy and arg == nmmo.action.Item:\n",
    "                        actions[i][atn][arg] = self.get_item_index(\n",
    "                            val, self.agents[i].ob.market)\n",
    "        return actions\n",
    "    \n",
    "    def act(self, observations, rewards=None):\n",
    "        '''\n",
    "        During training, pass rewards from last step to update RLlib policy.\n",
    "        \n",
    "        During inference (e.g. when make submission), no rewards need to be\n",
    "        passed to act().\n",
    "        '''\n",
    "        if \"stat\" in observations:\n",
    "            stat = observations.pop(\"stat\")\n",
    "            \n",
    "        if rewards is None:\n",
    "            # agent is training, register rewards\n",
    "            # if an action was chosen at the last step, assign team reward for taking that action\n",
    "            # TODO\n",
    "            pass\n",
    "        else:\n",
    "            # not training, no need to consider rewards\n",
    "            pass\n",
    "        \n",
    "        # TODO\n",
    "        # get team actions for this step\n",
    "        actions = {}\n",
    "        actions.update(self.get_scripted_agent_actions(observations))\n",
    "        actions.update(self.get_rllib_agent_actions(observations))\n",
    "        \n",
    "        # return team actions to TeamBasedEnv\n",
    "        return self.post_process_actions(actions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_item_index(instance: int, items: np.ndarray) -> int:\n",
    "        for i, itm in enumerate(items):\n",
    "            id_ = nmmo.scripting.Observation.attribute(itm,\n",
    "                                                       nmmo.Serialized.Item.ID)\n",
    "            if id_ == instance:\n",
    "                return i\n",
    "        raise ValueError(f\"Instance {instance} not found\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target_index(target: int, agents: np.ndarray) -> int:\n",
    "        targets = [\n",
    "            x for x in [\n",
    "                nmmo.scripting.Observation.attribute(\n",
    "                    agent, nmmo.Serialized.Entity.ID) for agent in agents\n",
    "            ] if x\n",
    "        ]\n",
    "        return targets.index(target)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# env config\n",
    "path_to_env_cls = 'neurips2022nmmo.TeamBasedEnv'\n",
    "path_to_env_config_cls = 'deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig'\n",
    "env_config_kwargs = None\n",
    "# teams_config = {\n",
    "#     'Combat':\n",
    "#         {\n",
    "#             'path_to_team_cls': 'neurips2022nmmo.scripted.CombatTeam'\n",
    "#         },\n",
    "#     'Mixture':\n",
    "#         {\n",
    "#             'path_to_team_cls': 'neurips2022nmmo.scripted.MixtureTeam'\n",
    "#         },\n",
    "# }\n",
    "rllib_paths_to_scripted_agents_cls = {\n",
    "                                    '0': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '1': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '2': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '3': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '4': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '5': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '6': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    }\n",
    "\n",
    "teams_config = {\n",
    "    'RLlib': {'cls': RLlibScriptedHybridAgentTeam, 'kwargs': {'paths_to_scripted_agents_cls': rllib_paths_to_scripted_agents_cls}},\n",
    "    'Combat': {'cls': neurips2022nmmo.scripted.CombatTeam, 'kwargs': {}},\n",
    "    'Mixture': {'cls': neurips2022nmmo.scripted.MixtureTeam, 'kwargs': {}},\n",
    "}\n",
    "teams_copies = [1, 2]        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# init env params\n",
    "env_config = get_class_from_path(path_to_env_config_cls)()\n",
    "\n",
    "teams = []\n",
    "for team_id, params in teams_config.items():\n",
    "    team_cls, team_kwargs = params['cls'], params['kwargs']\n",
    "    team_kwargs['env_config'] = env_config\n",
    "    team_kwargs['team_id'] = team_id\n",
    "    teams.append(team_cls(**team_kwargs))\n",
    "print(f'Teams: {teams}')\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    class Agent(nmmo.Agent):\n",
    "        name = f'{team.id}'\n",
    "        policy = f'{team.id}'\n",
    "    env_config.PLAYERS[i] = Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9830f-0a09-4ad1-8a94-2b9c2e78c47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0712292-f064-4b5c-b4f8-be35be7e6a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cda414-e36d-4e86-99cf-f6ff61d535c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "348ad6ba-6af9-4b32-8199-e69879de67ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agents: [<nmmo.entity.player.Player object at 0x7f543a832a60>, <nmmo.entity.player.Player object at 0x7f544fe559a0>, <nmmo.entity.player.Player object at 0x7f54502fe8e0>, <nmmo.entity.player.Player object at 0x7f5439637820>, <nmmo.entity.player.Player object at 0x7f5439624760>, <nmmo.entity.player.Player object at 0x7f54398b46a0>, <nmmo.entity.player.Player object at 0x7f54398a55e0>, <nmmo.entity.player.Player object at 0x7f5439a6e520>, <nmmo.entity.player.Player object at 0x7f5439a4c460>, <nmmo.entity.player.Player object at 0x7f543a0ef3a0>, <nmmo.entity.player.Player object at 0x7f543a1b62e0>, <nmmo.entity.player.Player object at 0x7f543a1a6220>, <nmmo.entity.player.Player object at 0x7f543a5b5160>, <nmmo.entity.player.Player object at 0x7f543a5a70a0>, <nmmo.entity.player.Player object at 0x7f543a5a7fa0>, <nmmo.entity.player.Player object at 0x7f544ff07ee0>, <nmmo.entity.player.Player object at 0x7f544feebe20>, <nmmo.entity.player.Player object at 0x7f544fffcd60>, <nmmo.entity.player.Player object at 0x7f545020aca0>, <nmmo.entity.player.Player object at 0x7f54501fcbe0>, <nmmo.entity.player.Player object at 0x7f545754fb20>, <nmmo.entity.player.Player object at 0x7f545751ea60>, <nmmo.entity.player.Player object at 0x7f54575699a0>, <nmmo.entity.player.Player object at 0x7f54575518e0>]\n",
      "_agent_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "<RLlibMultiAgentTeamBasedEnv instance>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m policies \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m policy_id \u001b[38;5;129;01min\u001b[39;00m dummy_env\u001b[38;5;241m.\u001b[39m_agent_ids:\n\u001b[1;32m     48\u001b[0m     policies[\u001b[38;5;28mstr\u001b[39m(policy_id)] \u001b[38;5;241m=\u001b[39m PolicySpec(\n\u001b[1;32m     49\u001b[0m                                 \u001b[38;5;66;03m# policy_class=player.__class__, # infer automatically from algorithm\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m                                 observation_space\u001b[38;5;241m=\u001b[39m\u001b[43mdummy_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     51\u001b[0m                                 action_space\u001b[38;5;241m=\u001b[39mdummy_env\u001b[38;5;241m.\u001b[39maction_space(player_id),\n\u001b[1;32m     52\u001b[0m                                 config\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m     53\u001b[0m                             )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy_mapping_fn\u001b[39m(agent_id, episode, worker, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124;03m'''Maps agent ID to corresponding policy ID of policy which agent should use.'''\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# set up configs\n",
    "\n",
    "# env config\n",
    "path_to_env_cls = 'neurips2022nmmo.TeamBasedEnv'\n",
    "path_to_env_config_cls = 'deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig'\n",
    "env_config_kwargs = None\n",
    "teams_config = {\n",
    "    'Combat':\n",
    "        {\n",
    "            'path_to_team_cls': 'neurips2022nmmo.scripted.CombatTeam'\n",
    "        },\n",
    "    'Mixture':\n",
    "        {\n",
    "            'path_to_team_cls': 'neurips2022nmmo.scripted.MixtureTeam'\n",
    "        },\n",
    "}\n",
    "teams_copies = [1, 2]\n",
    "\n",
    "# init env params\n",
    "env_config, teams_copies, teams = init_env_params(path_to_env_config_cls=path_to_env_config_cls,\n",
    "                                                  env_config_kwargs=env_config_kwargs,\n",
    "                                                  teams_copies=teams_copies,\n",
    "                                                  teams_config=teams_config)\n",
    "\n",
    "\n",
    "\n",
    "# rllib config\n",
    "path_to_rllib_trainer_cls = 'ray.rllib.agents.ppo.PPOTrainer'\n",
    "\n",
    "# ma_cls = make_multi_agent(lambda env_config: get_class_from_path(path_to_env_cls)(env_config))\n",
    "# print(ma_cls)\n",
    "# dummy_env = ma_cls(env_config)\n",
    "# print(dummy_env)\n",
    "# print(type(dummy_env))\n",
    "# _ = dummy_env.reset()\n",
    "\n",
    "\n",
    "dummy_env = RLlibMultiAgentTeamBasedEnv(\n",
    "                                    path_to_env_cls=path_to_env_cls,\n",
    "                                    path_to_env_config_cls=path_to_env_config_cls,\n",
    "                                    teams_copies=teams_copies,\n",
    "                                    teams_config=teams_config,\n",
    "                                )\n",
    "print(dummy_env)\n",
    "\n",
    "policies = {}\n",
    "for policy_id in dummy_env._agent_ids:\n",
    "    policies[str(policy_id)] = PolicySpec(\n",
    "                                # policy_class=player.__class__, # infer automatically from algorithm\n",
    "                                # observation_space=dummy_env.observation_space(player_id),\n",
    "                                # action_space=dummy_env.action_space(player_id),\n",
    "                                config={},\n",
    "                            )\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    '''Maps agent ID to corresponding policy ID of policy which agent should use.'''\n",
    "    return str(agent_id)\n",
    "\n",
    "multiagent_config = {\n",
    "    'policies': policies,\n",
    "    'policy_mapping_fn': policy_mapping_fn,\n",
    "}\n",
    "\n",
    "\n",
    "rllib_config = {\n",
    "    'framework': 'torch',\n",
    "    \n",
    "    'multiagent': multiagent_config,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce6872-6e7e-49a4-b798-bc1cac5ad442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54491403-ac9e-4141-8786-6470bfc14791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register env with ray\n",
    "register_env(path_to_env_cls.split('.')[-1], lambda env_config: make_multi_agent(get_class_from_path(path_to_env_cls))(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faf17572-0d23-49a2-9788-d706f95b412f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(AssertionError)",
     "evalue": "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=67793, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50>)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n    return func(self, *args, **kwargs)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n    self.foreach_worker(lambda w: w.assert_healthy())\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n    assert is_healthy, (\nAssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50> (idx=1; num_workers=2) not healthy!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AssertionError)\u001b[0m              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # update rllib config with observation and action spaces\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dummy_env = get_class_from_path(path_to_env_cls)(env_config)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# agent = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# init rllib trainer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_rllib_trainer_cls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrllib_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:308\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     }\n\u001b[1;32m    306\u001b[0m }\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:418\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:125\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_workers_after_construction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m     local_worker\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m ):\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:269\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". If not, the following will throw a RayError\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# which needs to be handled by this WorkerSet's owner (usually\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# a RLlib Algorithm instance).\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:391\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_worker() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     local_result \u001b[38;5;241m=\u001b[39m [func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_worker())]\n\u001b[0;32m--> 391\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_result \u001b[38;5;241m+\u001b[39m remote_results\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/_private/worker.py:2275\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AssertionError)\u001b[0m: \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=67793, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50>)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n    return func(self, *args, **kwargs)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n    self.foreach_worker(lambda w: w.assert_healthy())\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n    assert is_healthy, (\nAssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50> (idx=1; num_workers=2) not healthy!"
     ]
    }
   ],
   "source": [
    "# # update rllib config with observation and action spaces\n",
    "# dummy_env = get_class_from_path(path_to_env_cls)(env_config)\n",
    "# agent = 0\n",
    "# rllib_config['observation_space'] = dummy_env.observation_space(agent)\n",
    "# rllib_config['action_space'] = dummy_env.action_space(agent)\n",
    "\n",
    "# init rllib trainer\n",
    "trainer = get_class_from_path(path_to_rllib_trainer_cls)(config=rllib_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c3fa2-1835-4522-87a0-99a5e6f9b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TeamBasedEnv(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c153f72-dbf4-4fca-94db-2eb72509f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 13:22:29,162\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=67794, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f86f6178a90>)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n",
      "    self.foreach_worker(lambda w: w.assert_healthy())\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n",
      "    assert is_healthy, (\n",
      "AssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f86f6178a90> (idx=2; num_workers=2) not healthy!\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space(0))\n",
    "print(type(env.observation_space(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e838899-8439-44e7-8ca0-b64677d25f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 13:08:06,043\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=65655, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f07c37f8b50>)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n",
      "    self.foreach_worker(lambda w: w.assert_healthy())\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n",
      "    assert is_healthy, (\n",
      "AssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f07c37f8b50> (idx=2; num_workers=2) not healthy!\n"
     ]
    }
   ],
   "source": [
    "for key, val in env.observation_space(0).items():\n",
    "    print(f'\\nkey: {key}')\n",
    "    print(f'val: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9adc6-d631-40d7-ba53-bc6a8bc0dc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmmo",
   "language": "python",
   "name": "nmmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
