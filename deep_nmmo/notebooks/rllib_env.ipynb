{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80d450f6-d874-4883-ad86-df7f87dc0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:55:28,903\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from deep_nmmo.utils import get_class_from_path\n",
    "from deep_nmmo.envs.team_based_env.env_configs.custom_competition_config import CustomCompetitionConfig\n",
    "from deep_nmmo.envs.team_based_env.loops.utils import init_env_params\n",
    "\n",
    "import nmmo\n",
    "from nmmo import config\n",
    "from nmmo.io import action\n",
    "\n",
    "import neurips2022nmmo\n",
    "from neurips2022nmmo.scripted import baselines\n",
    "from neurips2022nmmo import Team\n",
    "from neurips2022nmmo import CompetitionConfig, scripted, RollOut, TeamBasedEnv\n",
    "\n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "# from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from typing import Dict, Any, Type, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b06c5-7760-4a2d-9ac6-8c6e1b09e7c6",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- To use own custom multi-agent environment, should use the team's perspective and implement inside RLlibTeam class\n",
    "- I.e. TeamBasedEnv is still the main env, but RLlibTeam class uses an RLlibMultiAgentTeamEnv \n",
    "\n",
    "PROBLEM: How are we going to register `RLlibMultiAgentTeamEnv` if it requires us to use the `TeamBasedEnv`? We cannot access internal RLlib env loop, so not sure this is possible. May be forced to somehow get `TeamBasedEnv` working with RLlib directly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5f45115-62f2-4030-a934-2becaa3ad000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team_id: RLlib | kwargs: {'paths_to_scripted_agents_cls': {'0': 'neurips2022nmmo.scripted.baselines.Mage', '1': 'neurips2022nmmo.scripted.baselines.Mage', '2': 'neurips2022nmmo.scripted.baselines.Mage', '3': 'neurips2022nmmo.scripted.baselines.Mage', '4': 'neurips2022nmmo.scripted.baselines.Mage', '5': 'neurips2022nmmo.scripted.baselines.Mage', '6': 'neurips2022nmmo.scripted.baselines.Mage'}}\n",
      "team_id: RLlib\n",
      "scripted_agents: [<neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc610>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2e0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2b0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc3a0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc310>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc400>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc190>]\n",
      "scripted_agent_ids: {0, 1, 2, 3, 4, 5, 6}\n",
      "scripted_agent_idxs: {0, 1, 2, 3, 4, 5, 6}\n",
      "rllib_agents: [None]\n",
      "rllib_agent_ids: {7}\n",
      "rllib_agent_idxs: {8}\n",
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space: Dict(Entity:Dict(Continuous:Box(-1048576.0, 1048576.0, (100, 24), float32), Discrete:Box(0, 4096, (100, 5), int32), N:Box(0, 100, (1,), int32)), Item:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Market:Dict(Continuous:Box(-1048576.0, 1048576.0, (170, 16), float32), Discrete:Box(0, 4096, (170, 3), int32), N:Box(0, 170, (1,), int32)), Tile:Dict(Continuous:Box(-1048576.0, 1048576.0, (225, 4), float32), Discrete:Box(0, 4096, (225, 3), int32), N:Box(0, 15, (1,), int32)))\n",
      "action_space: Dict(<class 'nmmo.io.action.Attack'>:Dict(<class 'nmmo.io.action.Style'>:Discrete(3), <class 'nmmo.io.action.Target'>:Discrete(100)), <class 'nmmo.io.action.Buy'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)), <class 'nmmo.io.action.Comm'>:Dict(<class 'nmmo.io.action.Token'>:Discrete(170)), <class 'nmmo.io.action.Move'>:Dict(<class 'nmmo.io.action.Direction'>:Discrete(4)), <class 'nmmo.io.action.Sell'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170), <class 'nmmo.io.action.Price'>:Discrete(100)), <class 'nmmo.io.action.Use'>:Dict(<class 'nmmo.io.action.Item'>:Discrete(170)))\n",
      "agents: [<neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc610>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2e0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc2b0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc3a0>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc310>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc400>, <neurips2022nmmo.scripted.baselines.Mage object at 0x7f252bfcc190>, None]\n",
      "_agent_ids: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "team_env: <RLlibMultiAgentTeamEnv instance>\n",
      "team_id: Combat | kwargs: {}\n",
      "team_id: Mixture | kwargs: {}\n",
      "Teams: [<__main__.RLlibScriptedHybridAgentTeam object at 0x7f252bfcc460>, <neurips2022nmmo.scripted.scripted_team.CombatTeam object at 0x7f2521e11fd0>, <neurips2022nmmo.scripted.scripted_team.MixtureTeam object at 0x7f2521e162b0>]\n"
     ]
    }
   ],
   "source": [
    "class RLlibMultiAgentTeamEnv(MultiAgentEnv):\n",
    "    def __init__(self, \n",
    "                 env_config,\n",
    "                 agent_ids_to_agents: dict,\n",
    "                 team_id,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        # inherit from RLlib multi-agent env\n",
    "        MultiAgentEnv.__init__(self)\n",
    "        \n",
    "        self.team_id = team_id # use for indexing to get team's params from TeamBasedEnv\n",
    "        \n",
    "        # init dummy env to get obs and action spaces\n",
    "        dummy_env = TeamBasedEnv(env_config)\n",
    "        self.observation_space = dummy_env.observation_space(self.team_id)\n",
    "        self.action_space = dummy_env.action_space(self.team_id)\n",
    "        print(f'observation_space: {self.observation_space}')\n",
    "        print(f'action_space: {self.action_space}')\n",
    "        \n",
    "        # init agents\n",
    "        self.agent_ids_to_agents = agent_ids_to_agents\n",
    "        self._agent_ids = list(agent_ids_to_agents.keys())\n",
    "        self.agents = list(agent_ids_to_agents.values())\n",
    "        print(f'agents: {self.agents}')\n",
    "        print(f'_agent_ids: {self._agent_ids}')\n",
    "        \n",
    "        self.dones = set()\n",
    "        \n",
    "    @override(MultiAgentEnv)\n",
    "    def observation_space_sample(self, agent_ids: list = None) -> MultiAgentDict:\n",
    "        if agent_ids is None:\n",
    "            agent_ids = list(range(len(self.agents)))\n",
    "        obs = {agent_id: self.observation_space.sample() for agent_id in agent_ids}\n",
    "\n",
    "        return obs\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def action_space_sample(self, agent_ids: list = None) -> MultiAgentDict:\n",
    "        if agent_ids is None:\n",
    "            agent_ids = list(range(len(self.agents)))\n",
    "        actions = {agent_id: self.action_space.sample() for agent_id in agent_ids}\n",
    "\n",
    "        return actions\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def action_space_contains(self, x: MultiAgentDict) -> bool:\n",
    "        if not isinstance(x, dict):\n",
    "            return False\n",
    "        return all(self.action_space.contains(val) for val in x.values())\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def observation_space_contains(self, x: MultiAgentDict) -> bool:\n",
    "        if not isinstance(x, dict):\n",
    "            return False\n",
    "        return all(self.observation_space.contains(val) for val in x.values())\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def reset(self):\n",
    "        self.dones = set()\n",
    "        return {i: a.reset() for i, a in enumerate(self.agents)}\n",
    "\n",
    "    @override(MultiAgentEnv)\n",
    "    def step(self, action_dict):\n",
    "        obs, rew, done, info = {}, {}, {}, {}\n",
    "        for i, action in action_dict.items():\n",
    "            obs[i], rew[i], done[i], info[i] = self.agents[i].step(action)\n",
    "            if done[i]:\n",
    "                self.dones.add(i)\n",
    "        done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "        return obs, rew, done, info\n",
    "\n",
    "class RLlibScriptedHybridAgentTeam(Team):\n",
    "    def __init__(self,\n",
    "                 team_id,\n",
    "                 env_config,\n",
    "                 paths_to_scripted_agents_cls,\n",
    "                 **kwargs):\n",
    "        super().__init__(team_id, env_config)\n",
    "        self.id = team_id\n",
    "        print(f'team_id: {self.id}')\n",
    "        \n",
    "        self.scripted_agents = [get_class_from_path(path_to_scripted_agent_cls)(config=env_config, idx=int(idx)) for idx, path_to_scripted_agent_cls in paths_to_scripted_agents_cls.items()]\n",
    "        self.scripted_agent_ids = set([int(idx) for idx in paths_to_scripted_agents_cls.keys()])\n",
    "        self.scripted_agent_idxs = set([idx for idx in range(len(self.scripted_agents))])\n",
    "        \n",
    "        self.rllib_agents = [None]\n",
    "        self.rllib_agent_ids = set([sorted(self.scripted_agent_ids)[-1]+1])\n",
    "        self.rllib_agent_idxs = set([len(self.scripted_agents)+idx for idx in range(1, len(self.rllib_agents)+1)])\n",
    "        \n",
    "        print(f'scripted_agents: {self.scripted_agents}')\n",
    "        print(f'scripted_agent_ids: {self.scripted_agent_ids}')\n",
    "        print(f'scripted_agent_idxs: {self.scripted_agent_idxs}')\n",
    "        \n",
    "        print(f'rllib_agents: {self.rllib_agents}')\n",
    "        print(f'rllib_agent_ids: {self.rllib_agent_ids}')\n",
    "        print(f'rllib_agent_idxs: {self.rllib_agent_idxs}')\n",
    "        \n",
    "        # init multi-agent env for team to interact with\n",
    "        agent_ids_to_agents = {}\n",
    "        for agent_id, agent in zip(self.scripted_agent_ids, self.scripted_agents):\n",
    "            agent_ids_to_agents[agent_id] = agent\n",
    "        for agent_id, agent in zip(self.rllib_agent_ids, self.rllib_agents):\n",
    "            agent_ids_to_agents[agent_id] = agent\n",
    "        self.team_env = RLlibMultiAgentTeamEnv(\n",
    "                                            env_config=env_config,\n",
    "                                            agent_ids_to_agents=agent_ids_to_agents,\n",
    "                                            team_id=self.id,\n",
    "                                        )\n",
    "        print(f'team_env: {self.team_env}')\n",
    "        \n",
    "    def reset(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def get_rllib_agent_actions(self, observations):\n",
    "        return {i: self.agents[i](obs) for i, obs in observations.items() if i in self.rllib_agent_idxs}\n",
    "    \n",
    "    def get_scripted_agent_actions(self, observations):\n",
    "        return {i: self.agents[i](obs) for i, obs in observations.items() if i in self.scripted_agent_idxs}\n",
    "    \n",
    "    def post_process_actions(self, actions):\n",
    "        for i in actions:\n",
    "            for atn, args in actions[i].items():\n",
    "                for arg, val in args.items():\n",
    "                    if arg.argType == nmmo.action.Fixed:\n",
    "                        actions[i][atn][arg] = arg.edges.index(val)\n",
    "                    elif arg == nmmo.action.Target:\n",
    "                        actions[i][atn][arg] = self.get_target_index(\n",
    "                            val, self.agents[i].ob.agents)\n",
    "                    elif atn in (nmmo.action.Sell,\n",
    "                                 nmmo.action.Use) and arg == nmmo.action.Item:\n",
    "                        actions[i][atn][arg] = self.get_item_index(\n",
    "                            val, self.agents[i].ob.items)\n",
    "                    elif atn == nmmo.action.Buy and arg == nmmo.action.Item:\n",
    "                        actions[i][atn][arg] = self.get_item_index(\n",
    "                            val, self.agents[i].ob.market)\n",
    "        return actions\n",
    "    \n",
    "    def act(self, observations, rewards=None):\n",
    "        '''\n",
    "        During training, pass rewards from last step to update RLlib policy.\n",
    "        \n",
    "        During inference (e.g. when make submission), no rewards need to be\n",
    "        passed to act().\n",
    "        '''\n",
    "        if \"stat\" in observations:\n",
    "            stat = observations.pop(\"stat\")\n",
    "            \n",
    "        if rewards is None:\n",
    "            # agent is training, register rewards\n",
    "            # if an action was chosen at the last step, assign team reward for taking that action\n",
    "            # TODO\n",
    "            pass\n",
    "        else:\n",
    "            # not training, no need to consider rewards\n",
    "            pass\n",
    "        \n",
    "        # TODO\n",
    "        # get team actions for this step\n",
    "        actions = {}\n",
    "        actions.update(self.get_scripted_agent_actions(observations))\n",
    "        actions.update(self.get_rllib_agent_actions(observations))\n",
    "        \n",
    "        # return team actions to TeamBasedEnv\n",
    "        return self.post_process_actions(actions)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_item_index(instance: int, items: np.ndarray) -> int:\n",
    "        for i, itm in enumerate(items):\n",
    "            id_ = nmmo.scripting.Observation.attribute(itm,\n",
    "                                                       nmmo.Serialized.Item.ID)\n",
    "            if id_ == instance:\n",
    "                return i\n",
    "        raise ValueError(f\"Instance {instance} not found\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target_index(target: int, agents: np.ndarray) -> int:\n",
    "        targets = [\n",
    "            x for x in [\n",
    "                nmmo.scripting.Observation.attribute(\n",
    "                    agent, nmmo.Serialized.Entity.ID) for agent in agents\n",
    "            ] if x\n",
    "        ]\n",
    "        return targets.index(target)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# env config\n",
    "path_to_env_cls = 'neurips2022nmmo.TeamBasedEnv'\n",
    "path_to_env_config_cls = 'deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig'\n",
    "env_config_kwargs = None\n",
    "# teams_config = {\n",
    "#     'Combat':\n",
    "#         {\n",
    "#             'path_to_team_cls': 'neurips2022nmmo.scripted.CombatTeam'\n",
    "#         },\n",
    "#     'Mixture':\n",
    "#         {\n",
    "#             'path_to_team_cls': 'neurips2022nmmo.scripted.MixtureTeam'\n",
    "#         },\n",
    "# }\n",
    "rllib_paths_to_scripted_agents_cls = {\n",
    "                                    '0': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '1': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '2': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '3': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '4': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '5': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    '6': 'neurips2022nmmo.scripted.baselines.Mage',\n",
    "                                    }\n",
    "\n",
    "teams_config = {\n",
    "    'RLlib': {'cls': RLlibScriptedHybridAgentTeam, 'kwargs': {'paths_to_scripted_agents_cls': rllib_paths_to_scripted_agents_cls}},\n",
    "    'Combat': {'cls': neurips2022nmmo.scripted.CombatTeam, 'kwargs': {}},\n",
    "    'Mixture': {'cls': neurips2022nmmo.scripted.MixtureTeam, 'kwargs': {}},\n",
    "}\n",
    "teams_copies = [1, 2]        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# init env params\n",
    "env_config = get_class_from_path(path_to_env_config_cls)()\n",
    "\n",
    "teams = []\n",
    "for team_id, params in teams_config.items():\n",
    "    team_cls, team_kwargs = params['cls'], params['kwargs']\n",
    "    team_kwargs['env_config'] = env_config\n",
    "    team_kwargs['team_id'] = team_id\n",
    "    teams.append(team_cls(**team_kwargs))\n",
    "print(f'Teams: {teams}')\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    class Agent(nmmo.Agent):\n",
    "        name = f'{team.id}'\n",
    "        policy = f'{team.id}'\n",
    "    env_config.PLAYERS[i] = Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9830f-0a09-4ad1-8a94-2b9c2e78c47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0712292-f064-4b5c-b4f8-be35be7e6a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cda414-e36d-4e86-99cf-f6ff61d535c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "348ad6ba-6af9-4b32-8199-e69879de67ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▎                                                                                                                                                              | 2/40 [00:00<00:02, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 40 maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agents: [<nmmo.entity.player.Player object at 0x7f543a832a60>, <nmmo.entity.player.Player object at 0x7f544fe559a0>, <nmmo.entity.player.Player object at 0x7f54502fe8e0>, <nmmo.entity.player.Player object at 0x7f5439637820>, <nmmo.entity.player.Player object at 0x7f5439624760>, <nmmo.entity.player.Player object at 0x7f54398b46a0>, <nmmo.entity.player.Player object at 0x7f54398a55e0>, <nmmo.entity.player.Player object at 0x7f5439a6e520>, <nmmo.entity.player.Player object at 0x7f5439a4c460>, <nmmo.entity.player.Player object at 0x7f543a0ef3a0>, <nmmo.entity.player.Player object at 0x7f543a1b62e0>, <nmmo.entity.player.Player object at 0x7f543a1a6220>, <nmmo.entity.player.Player object at 0x7f543a5b5160>, <nmmo.entity.player.Player object at 0x7f543a5a70a0>, <nmmo.entity.player.Player object at 0x7f543a5a7fa0>, <nmmo.entity.player.Player object at 0x7f544ff07ee0>, <nmmo.entity.player.Player object at 0x7f544feebe20>, <nmmo.entity.player.Player object at 0x7f544fffcd60>, <nmmo.entity.player.Player object at 0x7f545020aca0>, <nmmo.entity.player.Player object at 0x7f54501fcbe0>, <nmmo.entity.player.Player object at 0x7f545754fb20>, <nmmo.entity.player.Player object at 0x7f545751ea60>, <nmmo.entity.player.Player object at 0x7f54575699a0>, <nmmo.entity.player.Player object at 0x7f54575518e0>]\n",
      "_agent_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "<RLlibMultiAgentTeamBasedEnv instance>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m policies \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m policy_id \u001b[38;5;129;01min\u001b[39;00m dummy_env\u001b[38;5;241m.\u001b[39m_agent_ids:\n\u001b[1;32m     48\u001b[0m     policies[\u001b[38;5;28mstr\u001b[39m(policy_id)] \u001b[38;5;241m=\u001b[39m PolicySpec(\n\u001b[1;32m     49\u001b[0m                                 \u001b[38;5;66;03m# policy_class=player.__class__, # infer automatically from algorithm\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m                                 observation_space\u001b[38;5;241m=\u001b[39m\u001b[43mdummy_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     51\u001b[0m                                 action_space\u001b[38;5;241m=\u001b[39mdummy_env\u001b[38;5;241m.\u001b[39maction_space(player_id),\n\u001b[1;32m     52\u001b[0m                                 config\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m     53\u001b[0m                             )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy_mapping_fn\u001b[39m(agent_id, episode, worker, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124;03m'''Maps agent ID to corresponding policy ID of policy which agent should use.'''\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# set up configs\n",
    "\n",
    "# env config\n",
    "path_to_env_cls = 'neurips2022nmmo.TeamBasedEnv'\n",
    "path_to_env_config_cls = 'deep_nmmo.envs.team_based_env.env_configs.custom_competition_config.CustomCompetitionConfig'\n",
    "env_config_kwargs = None\n",
    "teams_config = {\n",
    "    'Combat':\n",
    "        {\n",
    "            'path_to_team_cls': 'neurips2022nmmo.scripted.CombatTeam'\n",
    "        },\n",
    "    'Mixture':\n",
    "        {\n",
    "            'path_to_team_cls': 'neurips2022nmmo.scripted.MixtureTeam'\n",
    "        },\n",
    "}\n",
    "teams_copies = [1, 2]\n",
    "\n",
    "# init env params\n",
    "env_config, teams_copies, teams = init_env_params(path_to_env_config_cls=path_to_env_config_cls,\n",
    "                                                  env_config_kwargs=env_config_kwargs,\n",
    "                                                  teams_copies=teams_copies,\n",
    "                                                  teams_config=teams_config)\n",
    "\n",
    "\n",
    "\n",
    "# rllib config\n",
    "path_to_rllib_trainer_cls = 'ray.rllib.agents.ppo.PPOTrainer'\n",
    "\n",
    "# ma_cls = make_multi_agent(lambda env_config: get_class_from_path(path_to_env_cls)(env_config))\n",
    "# print(ma_cls)\n",
    "# dummy_env = ma_cls(env_config)\n",
    "# print(dummy_env)\n",
    "# print(type(dummy_env))\n",
    "# _ = dummy_env.reset()\n",
    "\n",
    "\n",
    "dummy_env = RLlibMultiAgentTeamBasedEnv(\n",
    "                                    path_to_env_cls=path_to_env_cls,\n",
    "                                    path_to_env_config_cls=path_to_env_config_cls,\n",
    "                                    teams_copies=teams_copies,\n",
    "                                    teams_config=teams_config,\n",
    "                                )\n",
    "print(dummy_env)\n",
    "\n",
    "policies = {}\n",
    "for policy_id in dummy_env._agent_ids:\n",
    "    policies[str(policy_id)] = PolicySpec(\n",
    "                                # policy_class=player.__class__, # infer automatically from algorithm\n",
    "                                # observation_space=dummy_env.observation_space(player_id),\n",
    "                                # action_space=dummy_env.action_space(player_id),\n",
    "                                config={},\n",
    "                            )\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    '''Maps agent ID to corresponding policy ID of policy which agent should use.'''\n",
    "    return str(agent_id)\n",
    "\n",
    "multiagent_config = {\n",
    "    'policies': policies,\n",
    "    'policy_mapping_fn': policy_mapping_fn,\n",
    "}\n",
    "\n",
    "\n",
    "rllib_config = {\n",
    "    'framework': 'torch',\n",
    "    \n",
    "    'multiagent': multiagent_config,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce6872-6e7e-49a4-b798-bc1cac5ad442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54491403-ac9e-4141-8786-6470bfc14791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register env with ray\n",
    "register_env(path_to_env_cls.split('.')[-1], lambda env_config: make_multi_agent(get_class_from_path(path_to_env_cls))(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faf17572-0d23-49a2-9788-d706f95b412f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(AssertionError)",
     "evalue": "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=67793, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50>)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n    return func(self, *args, **kwargs)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n    self.foreach_worker(lambda w: w.assert_healthy())\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n    assert is_healthy, (\nAssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50> (idx=1; num_workers=2) not healthy!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AssertionError)\u001b[0m              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # update rllib config with observation and action spaces\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dummy_env = get_class_from_path(path_to_env_cls)(env_config)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# agent = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# init rllib trainer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_rllib_trainer_cls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrllib_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:308\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     }\n\u001b[1;32m    306\u001b[0m }\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:418\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:125\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_workers_after_construction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m     local_worker\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m ):\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:269\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Validate here, whether all remote workers have been constructed properly\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# and are \"up and running\". If not, the following will throw a RayError\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# which needs to be handled by this WorkerSet's owner (usually\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# a RLlib Algorithm instance).\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:391\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_worker() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     local_result \u001b[38;5;241m=\u001b[39m [func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_worker())]\n\u001b[0;32m--> 391\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_result \u001b[38;5;241m+\u001b[39m remote_results\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/_private/worker.py:2275\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AssertionError)\u001b[0m: \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=67793, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50>)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n    return func(self, *args, **kwargs)\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n    self.foreach_worker(lambda w: w.assert_healthy())\n  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n    assert is_healthy, (\nAssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f6846fb50> (idx=1; num_workers=2) not healthy!"
     ]
    }
   ],
   "source": [
    "# # update rllib config with observation and action spaces\n",
    "# dummy_env = get_class_from_path(path_to_env_cls)(env_config)\n",
    "# agent = 0\n",
    "# rllib_config['observation_space'] = dummy_env.observation_space(agent)\n",
    "# rllib_config['action_space'] = dummy_env.action_space(agent)\n",
    "\n",
    "# init rllib trainer\n",
    "trainer = get_class_from_path(path_to_rllib_trainer_cls)(config=rllib_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c3fa2-1835-4522-87a0-99a5e6f9b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TeamBasedEnv(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c153f72-dbf4-4fca-94db-2eb72509f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 13:22:29,162\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=67794, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f86f6178a90>)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n",
      "    self.foreach_worker(lambda w: w.assert_healthy())\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n",
      "    assert is_healthy, (\n",
      "AssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f86f6178a90> (idx=2; num_workers=2) not healthy!\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space(0))\n",
    "print(type(env.observation_space(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e838899-8439-44e7-8ca0-b64677d25f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 13:08:06,043\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=65655, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f07c37f8b50>)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1664, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 269, in <lambda>\n",
      "    self.foreach_worker(lambda w: w.assert_healthy())\n",
      "  File \"/scratch/zciccwf/py36/envs/nmmo/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in assert_healthy\n",
      "    assert is_healthy, (\n",
      "AssertionError: RolloutWorker <ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f07c37f8b50> (idx=2; num_workers=2) not healthy!\n"
     ]
    }
   ],
   "source": [
    "for key, val in env.observation_space(0).items():\n",
    "    print(f'\\nkey: {key}')\n",
    "    print(f'val: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9adc6-d631-40d7-ba53-bc6a8bc0dc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmmo",
   "language": "python",
   "name": "nmmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
